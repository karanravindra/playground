{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 64])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"brackets.txt\", \"r\") as f:\n",
    "    brackets = f.readlines()\n",
    "idxs = list(map(lambda line: list(map(lambda val: int(val), line.split())), brackets))\n",
    "idxs = torch.tensor(idxs, dtype=torch.long)\n",
    "idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = 0.8\n",
    "train_size = int(train_val_split * len(idxs))\n",
    "\n",
    "train_idxs = idxs[:train_size]\n",
    "val_idxs = idxs[train_size:]\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_idxs[:, :-1], train_idxs[:, 1:])\n",
    "val_dataset = torch.utils.data.TensorDataset(val_idxs[:, :-1], val_idxs[:, 1:])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SOS [<{[[<{{[([[[[<{<<<[({(<<(((<<{}>>)))>>)})]>>>}>]]]])]}}>]]}>] EOS'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_char = {\n",
    "    0: \"{\",\n",
    "    1: \"(\",\n",
    "    2: \"[\",\n",
    "    3: \"<\",\n",
    "    4: \"}\",\n",
    "    5: \")\",\n",
    "    6: \"]\",\n",
    "    7: \">\",\n",
    "    8: \"SOS \",\n",
    "    9: \" EOS\",\n",
    "    10: \"_\"\n",
    "}\n",
    "char_to_idx = {v: k for k, v in idx_to_char.items()}\n",
    "def decode_brackets(brackets):\n",
    "    brackets = brackets.tolist()\n",
    "    return \"\".join([idx_to_char[idx] for idx in brackets])\n",
    "\n",
    "decode_brackets(idxs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def show_diff(seq1, seq2):\n",
    "    diff = difflib.ndiff(seq1, seq2)\n",
    "    diff = list(diff)\n",
    "    print(\"\".join(seq1), end=\"\")\n",
    "    diff = reversed(diff)\n",
    "    for d in diff:\n",
    "        if d[0] == \"-\":\n",
    "            idx = char_to_idx[d[1:].strip()] + 4\n",
    "            print(f\"\\033[91m{idx_to_char[idx]}\\033[0m\", end=\"\")\n",
    "        elif d[0] == \"+\":\n",
    "            idx = char_to_idx[d[1:].strip()] + 4\n",
    "            print(f\"\\033[92m{idx_to_char[idx]}\\033[0m\", end=\"\")\n",
    "        else:\n",
    "            idx = char_to_idx[d.strip()] + 4\n",
    "            print(idx_to_char[idx], end=\"\")\n",
    "\n",
    "def compute_diff(seq):\n",
    "    seq = decode_brackets(seq).strip(\"_\").strip(\"[SOS] \").strip(\" [EOS]\")\n",
    "    print(seq)\n",
    "    brack_open = seq[:len(seq) // 2]\n",
    "    id_open = torch.tensor([char_to_idx[char] for char in brack_open], dtype=torch.long)\n",
    "    id_close = reversed(torch.tensor([char_to_idx[char] for char in seq[id_open.shape[0]:]], dtype=torch.long)) - 4\n",
    "    show_diff([idx_to_char[idx] for idx in id_open.tolist()], [idx_to_char[idx] for idx in id_close.tolist()])\n",
    "\n",
    "\n",
    "# sequence1 = \"<{[[<{{[([[[[<{<<<[({(\"\n",
    "# sequence2 = \"{[[<{{[([[[[<{<<<[((((\"\n",
    "\n",
    "# show_diff(sequence1, sequence2)\n",
    "\n",
    "compute_diff(idxs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in data_loader:\n",
    "        x, y = inputs.to(device), targets.to(device)\n",
    "\n",
    "        y_pred = model(x)\n",
    "        loss = model.loss(y_pred, y)\n",
    "        total_loss += loss\n",
    "       \n",
    "    return total_loss.item() / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    kv_cache: dict[str, torch.Tensor] | None\n",
    "\n",
    "    def __init__(self, n_embd: int, n_head: int, attn_dropout: float = 0.0, is_causal: bool = True):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0, f\"Embedding dimension {n_embd} should be divisible by number of heads {n_head}\"\n",
    "\n",
    "        self.kv_cache = None\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embd, n_embd * 3)\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.is_causal = is_causal\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        if self.kv_cache is None or self.kv_cache[\"k\"].shape[0] == 0:\n",
    "            qkv = self.c_attn(x)\n",
    "            q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "\n",
    "            # (B, T, C) -> (B, T, n_head, C // n_head) -> (B, n_head, T, C // n_head)\n",
    "            k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "            q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "            v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "            y = (\n",
    "                F.scaled_dot_product_attention(\n",
    "                    q, k, v, is_causal=self.is_causal, dropout_p=self.attn_dropout\n",
    "                )\n",
    "                .transpose(1, 2)\n",
    "                .contiguous()\n",
    "                .view(B, T, C)\n",
    "            )\n",
    "\n",
    "            # output projection\n",
    "            y = self.c_proj(y)\n",
    "\n",
    "            if self.kv_cache is not None:\n",
    "                # print(\"Setting cache\")\n",
    "                self.kv_cache[\"k\"] = k\n",
    "                self.kv_cache[\"v\"] = v\n",
    "\n",
    "            return y\n",
    "        \n",
    "        else:\n",
    "            # print(\"Using cache\")\n",
    "            if self.kv_cache[\"k\"].shape[0] != B:\n",
    "                self.kv_cache[\"k\"] = self.kv_cache[\"k\"][:B]\n",
    "                self.kv_cache[\"v\"] = self.kv_cache[\"v\"][:B]\n",
    "            \n",
    "            qkv = self.c_attn(x)\n",
    "            q, _, _ = qkv.split(self.n_embd, dim=2)\n",
    "            q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "            k = self.kv_cache[\"k\"].to(q.device)\n",
    "            v = self.kv_cache[\"v\"].to(q.device)\n",
    "            # print(q.shape, k.shape, v.shape)\n",
    "            y = (\n",
    "                F.scaled_dot_product_attention(\n",
    "                    q, k, v, is_causal=self.is_causal, dropout_p=self.attn_dropout\n",
    "                )\n",
    "                .transpose(1, 2)\n",
    "                .contiguous()\n",
    "                .view(B, T, C)\n",
    "            )\n",
    "\n",
    "            # output projection\n",
    "            y = self.c_proj(y)\n",
    "            return y\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, emb_dim, atten_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            SelfAttention(emb_dim, num_heads, atten_dropout) for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(emb_dim * num_heads, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        heads = [head(x) for head in self.heads]\n",
    "        x = torch.cat(heads, dim=-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, m=4):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            MultiHeadAttention(num_heads, emb_dim),\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim, m * emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(m * emb_dim, emb_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.fc(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab=11, emb_dim=64, seq_len=64):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.emb_dim = emb_dim\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab, emb_dim)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(seq_len, emb_dim))\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            TransformerBlock(emb_dim, num_heads=1, m=1),\n",
    "            TransformerBlock(emb_dim, num_heads=2, m=1),\n",
    "            TransformerBlock(emb_dim, num_heads=4, m=2),\n",
    "            TransformerBlock(emb_dim, num_heads=8, m=2),\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(emb_dim, vocab, bias=False)\n",
    "        \n",
    "        # init weights\n",
    "        self.lm_head.weight = self.tok_emb.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[1] > self.seq_len:\n",
    "            x = x[:, -self.seq_len:]\n",
    "        \n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb[:tok_emb.shape[1]]\n",
    "        \n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, y_pred, y):\n",
    "        return torch.nn.functional.cross_entropy(y_pred.reshape(-1, self.vocab), y.reshape(-1))\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, start: list[int] | torch.Tensor | None = None, max_len: int = 100, temperature: float = 1.0, top_k: int = 0, use_cache: bool = False):\n",
    "        if start is None:\n",
    "            start = torch.randint(self.vocab, (1, 1), device=device)\n",
    "        elif isinstance(start, list):\n",
    "            start = torch.tensor(start, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        if use_cache:\n",
    "            self.toggle_kv_cache(True)\n",
    "\n",
    "        x = start\n",
    "\n",
    "        for _ in tqdm(range(max_len)):\n",
    "            y_pred = self(x)\n",
    "            y_pred = y_pred[:, -1, :] / temperature\n",
    "            if top_k > 0:\n",
    "                y_pred = torch.topk(y_pred, top_k, dim=-1).values\n",
    "            next_char = torch.multinomial(torch.nn.functional.softmax(y_pred, dim=-1), 1)\n",
    "            x = torch.cat([x, next_char], dim=1)\n",
    "        \n",
    "        self.toggle_kv_cache(False)\n",
    "        return x\n",
    "    \n",
    "    def toggle_kv_cache(self, value: bool):\n",
    "        if value:\n",
    "            self.blocks.apply(lambda module: setattr(module, \"kv_cache\", {\"k\": torch.empty(0), \"v\": torch.empty(0)}))\n",
    "        else:\n",
    "            self.blocks.apply(lambda module: setattr(module, \"kv_cache\", None))\n",
    "\n",
    "model = Model().to(device)\n",
    "# model.toggle_kv_cache(True)\n",
    "print(decode_brackets(model.generate(max_len=4, use_cache=False).squeeze()))\n",
    "print(f\"Evaluation loss: {evaluate(model, val_loader):.4f}\")\n",
    "summary(model, (1, 63), dtypes=[torch.long], device=device, depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"val_loss\" in locals():\n",
    "    pass\n",
    "else:\n",
    "    val_loss = float(\"inf\")\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = model.loss(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item(), val_loss=val_loss)\n",
    "\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    print(f\"Validation loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_diff(model.generate(max_len=64).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualize embeddings\n",
    "with torch.no_grad():\n",
    "    emb = model.tok_emb.weight.cpu().numpy()\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(emb, cmap=\"viridis\")\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Embeddings\")\n",
    "    plt.xlabel(\"Embedding dimension\")\n",
    "    plt.ylabel(\"Vocabulary index\")\n",
    "    plt.yticks(list(idx_to_char.keys()), list(idx_to_char.values()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
