{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 64])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"brackets.txt\", \"r\") as f:\n",
    "    brackets = f.readlines()\n",
    "idxs = list(map(lambda line: list(map(lambda val: int(val), line.split())), brackets))\n",
    "idxs = torch.tensor(idxs, dtype=torch.long)\n",
    "idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = 0.8\n",
    "train_size = int(train_val_split * len(idxs))\n",
    "\n",
    "train_idxs = idxs[:train_size]\n",
    "val_idxs = idxs[train_size:]\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_idxs[:, :-1], train_idxs[:, 1:])\n",
    "val_dataset = torch.utils.data.TensorDataset(val_idxs[:, :-1], val_idxs[:, 1:])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SOS [<{[[<{{[([[[[<{<<<[({(<<(((<<{}>>)))>>)})]>>>}>]]]])]}}>]]}>] EOS'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_char = {\n",
    "    0: \"{\",\n",
    "    1: \"(\",\n",
    "    2: \"[\",\n",
    "    3: \"<\",\n",
    "    4: \"}\",\n",
    "    5: \")\",\n",
    "    6: \"]\",\n",
    "    7: \">\",\n",
    "    8: \"SOS \",\n",
    "    9: \" EOS\",\n",
    "    10: \"_\"\n",
    "}\n",
    "char_to_idx = {v: k for k, v in idx_to_char.items()}\n",
    "def decode_brackets(brackets):\n",
    "    brackets = brackets.tolist()\n",
    "    return \"\".join([idx_to_char[idx] for idx in brackets])\n",
    "\n",
    "decode_brackets(idxs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<{[[<{{[([[[[<{<<<[({(<<(((<<{}>>)))>>)})]>>>}>]]]])]}}>]]}>\n",
      "<{[[<{{[([[[[<{<<<[({(<<(((<<{}>>)))>>)})]>>>}>]]]])]}}>]]}>"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "def show_diff(seq1, seq2):\n",
    "    diff = difflib.ndiff(seq1, seq2)\n",
    "    diff = list(diff)\n",
    "    print(\"\".join(seq1), end=\"\")\n",
    "    diff = reversed(diff)\n",
    "    for d in diff:\n",
    "        if d[0] == \"-\":\n",
    "            idx = char_to_idx[d[1:].strip()] + 4\n",
    "            print(f\"\\033[91m{idx_to_char[idx]}\\033[0m\", end=\"\")\n",
    "        elif d[0] == \"+\":\n",
    "            idx = char_to_idx[d[1:].strip()] + 4\n",
    "            print(f\"\\033[92m{idx_to_char[idx]}\\033[0m\", end=\"\")\n",
    "        else:\n",
    "            idx = char_to_idx[d.strip()] + 4\n",
    "            print(idx_to_char[idx], end=\"\")\n",
    "\n",
    "def compute_diff(seq):\n",
    "    seq = decode_brackets(seq).strip(\"_\").strip(\"[SOS] \").strip(\" [EOS]\")\n",
    "    print(seq)\n",
    "    brack_open = seq[:len(seq) // 2]\n",
    "    id_open = torch.tensor([char_to_idx[char] for char in brack_open], dtype=torch.long)\n",
    "    id_close = reversed(torch.tensor([char_to_idx[char] for char in seq[id_open.shape[0]:]], dtype=torch.long)) - 4\n",
    "    show_diff([idx_to_char[idx] for idx in id_open.tolist()], [idx_to_char[idx] for idx in id_close.tolist()])\n",
    "\n",
    "\n",
    "# sequence1 = \"<{[[<{{[([[[[<{<<<[({(\"\n",
    "# sequence2 = \"{[[<{{[([[[[<{<<<[((((\"\n",
    "\n",
    "# show_diff(sequence1, sequence2)\n",
    "\n",
    "compute_diff(idxs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in data_loader:\n",
    "        x, y = inputs.to(device), targets.to(device)\n",
    "\n",
    "        y_pred = model(x)\n",
    "        loss = model.loss(y_pred, y)\n",
    "        total_loss += loss\n",
    "       \n",
    "    return total_loss.item() / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 59.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "}<><_\n",
      "Evaluation loss: 2.3978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "Model                                              [1, 63, 11]               424\n",
       "├─Embedding: 1-1                                   [1, 63, 8]                88\n",
       "├─Sequential: 1-2                                  --                        --\n",
       "│    └─TransformerBlock: 2-1                       [1, 63, 8]                536\n",
       "│    └─TransformerBlock: 2-2                       [1, 63, 8]                888\n",
       "│    └─TransformerBlock: 2-3                       [1, 63, 8]                1,728\n",
       "│    └─TransformerBlock: 2-4                       [1, 63, 8]                3,136\n",
       "├─Linear: 1-3                                      [1, 63, 11]               88\n",
       "====================================================================================================\n",
       "Total params: 6,888\n",
       "Trainable params: 6,888\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.01\n",
       "====================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.34\n",
       "Params size (MB): 0.03\n",
       "Estimated Total Size (MB): 0.37\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    kv_cache: dict[str, torch.Tensor] | None\n",
    "\n",
    "    def __init__(self, n_embd: int, n_head: int, attn_dropout: float = 0.0, is_causal: bool = True):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0, f\"Embedding dimension {n_embd} should be divisible by number of heads {n_head}\"\n",
    "\n",
    "        self.kv_cache = None\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embd, n_embd * 3)\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.is_causal = is_causal\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        if self.kv_cache is None or self.kv_cache[\"k\"].shape[0] == 0:\n",
    "            qkv = self.c_attn(x)\n",
    "            q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "\n",
    "            # (B, T, C) -> (B, T, n_head, C // n_head) -> (B, n_head, T, C // n_head)\n",
    "            k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "            q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "            v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "            y = (\n",
    "                F.scaled_dot_product_attention(\n",
    "                    q, k, v, is_causal=self.is_causal, dropout_p=self.attn_dropout\n",
    "                )\n",
    "                .transpose(1, 2)\n",
    "                .contiguous()\n",
    "                .view(B, T, C)\n",
    "            )\n",
    "\n",
    "            # output projection\n",
    "            y = self.c_proj(y)\n",
    "\n",
    "            if self.kv_cache is not None:\n",
    "                # print(\"Setting cache\")\n",
    "                self.kv_cache[\"k\"] = k\n",
    "                self.kv_cache[\"v\"] = v\n",
    "\n",
    "            return y\n",
    "        \n",
    "        else:\n",
    "            # print(\"Using cache\")\n",
    "            if self.kv_cache[\"k\"].shape[0] != B:\n",
    "                self.kv_cache[\"k\"] = self.kv_cache[\"k\"][:B]\n",
    "                self.kv_cache[\"v\"] = self.kv_cache[\"v\"][:B]\n",
    "            \n",
    "            qkv = self.c_attn(x)\n",
    "            q, _, _ = qkv.split(self.n_embd, dim=2)\n",
    "            q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "            k = self.kv_cache[\"k\"].to(q.device)\n",
    "            v = self.kv_cache[\"v\"].to(q.device)\n",
    "            # print(q.shape, k.shape, v.shape)\n",
    "            y = (\n",
    "                F.scaled_dot_product_attention(\n",
    "                    q, k, v, is_causal=self.is_causal, dropout_p=self.attn_dropout\n",
    "                )\n",
    "                .transpose(1, 2)\n",
    "                .contiguous()\n",
    "                .view(B, T, C)\n",
    "            )\n",
    "\n",
    "            # output projection\n",
    "            y = self.c_proj(y)\n",
    "            return y\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, emb_dim, atten_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            SelfAttention(emb_dim, num_heads, atten_dropout) for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(emb_dim * num_heads, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        heads = [head(x) for head in self.heads]\n",
    "        x = torch.cat(heads, dim=-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, m=4):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            MultiHeadAttention(num_heads, emb_dim),\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim, m * emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(m * emb_dim, emb_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.fc(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab=11, emb_dim=8, seq_len=64):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.emb_dim = emb_dim\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab, emb_dim)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(seq_len, emb_dim))\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            TransformerBlock(emb_dim, num_heads=1, m=1),\n",
    "            TransformerBlock(emb_dim, num_heads=2, m=1),\n",
    "            TransformerBlock(emb_dim, num_heads=4, m=2),\n",
    "            TransformerBlock(emb_dim, num_heads=8, m=2),\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(emb_dim, vocab, bias=False)\n",
    "        \n",
    "        # init weights\n",
    "        self.lm_head.weight = self.tok_emb.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[1] > self.seq_len:\n",
    "            x = x[:, -self.seq_len:]\n",
    "        \n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb[:tok_emb.shape[1]]\n",
    "        \n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, y_pred, y):\n",
    "        return torch.nn.functional.cross_entropy(y_pred.reshape(-1, self.vocab), y.reshape(-1))\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, start: list[int] | torch.Tensor | None = None, max_len: int = 100, temperature: float = 1.0, top_k: int = 0, use_cache: bool = False):\n",
    "        if start is None:\n",
    "            start = torch.randint(self.vocab, (1, 1), device=device)\n",
    "        elif isinstance(start, list):\n",
    "            start = torch.tensor(start, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        if use_cache:\n",
    "            self.toggle_kv_cache(True)\n",
    "\n",
    "        x = start\n",
    "\n",
    "        for _ in tqdm(range(max_len)):\n",
    "            y_pred = self(x)\n",
    "            y_pred = y_pred[:, -1, :] / temperature\n",
    "            if top_k > 0:\n",
    "                y_pred = torch.topk(y_pred, top_k, dim=-1).values\n",
    "            next_char = torch.multinomial(torch.nn.functional.softmax(y_pred, dim=-1), 1)\n",
    "            x = torch.cat([x, next_char], dim=1)\n",
    "        \n",
    "        self.toggle_kv_cache(False)\n",
    "        return x\n",
    "    \n",
    "    def toggle_kv_cache(self, value: bool):\n",
    "        if value:\n",
    "            self.blocks.apply(lambda module: setattr(module, \"kv_cache\", {\"k\": torch.empty(0), \"v\": torch.empty(0)}))\n",
    "        else:\n",
    "            self.blocks.apply(lambda module: setattr(module, \"kv_cache\", None))\n",
    "\n",
    "model = Model().to(device)\n",
    "# model.toggle_kv_cache(True)\n",
    "print(decode_brackets(model.generate(max_len=4, use_cache=True).squeeze()))\n",
    "print(f\"Evaluation loss: {evaluate(model, val_loader):.4f}\")\n",
    "summary(model, (1, 63), dtypes=[torch.long], device=device, depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "`fused=True` requires all the params to be floating point Tensors of supported devices: ['cuda', 'xpu', 'privateuseone'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/playground/.venv/lib/python3.11/site-packages/torch/optim/adam.py:60\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     55\u001b[0m fused_supported_devices \u001b[38;5;241m=\u001b[39m _get_fused_kernels_supported_devices()\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m     57\u001b[0m     p\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01min\u001b[39;00m fused_supported_devices \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     torch\u001b[38;5;241m.\u001b[39mis_floating_point(p) \u001b[38;5;28;01mfor\u001b[39;00m pg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m pg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     59\u001b[0m ):\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fused=True` requires all the params to be floating point Tensors of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported devices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfused_supported_devices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m foreach:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fused` and `foreach` cannot be `True` together.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: `fused=True` requires all the params to be floating point Tensors of supported devices: ['cuda', 'xpu', 'privateuseone']."
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, fused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.3.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: /Users/karan/projects/playground/.venv/lib/python3.11/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: lightning, nn-zoo, pytorch-lightning, torchmetrics, torchvision\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  47%|████▋     | 88/188 [00:17<00:20,  4.94it/s, loss=1.68, val_loss=1.36]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 19\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, val_loss\u001b[38;5;241m=\u001b[39mval_loss)\n\u001b[1;32m     21\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if \"val_loss\" in locals():\n",
    "    pass\n",
    "else:\n",
    "    val_loss = float(\"inf\")\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = model.loss(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item(), val_loss=val_loss)\n",
    "\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    print(f\"Validation loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 86.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "><{({{<((<{(<[<<(<{(<{<({<<{<{[(>}}>)]]>>>}>}]>>]>})>]}}])}}]>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "-3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompute_diff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36mcompute_diff\u001b[0;34m(seq)\u001b[0m\n\u001b[1;32m     23\u001b[0m id_open \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([char_to_idx[char] \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m brack_open], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     24\u001b[0m id_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mreversed\u001b[39m(torch\u001b[38;5;241m.\u001b[39mtensor([char_to_idx[char] \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m seq[id_open\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m---> 25\u001b[0m show_diff([idx_to_char[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m id_open\u001b[38;5;241m.\u001b[39mtolist()], \u001b[43m[\u001b[49m\u001b[43midx_to_char\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mid_close\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     23\u001b[0m id_open \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([char_to_idx[char] \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m brack_open], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     24\u001b[0m id_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mreversed\u001b[39m(torch\u001b[38;5;241m.\u001b[39mtensor([char_to_idx[char] \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m seq[id_open\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m---> 25\u001b[0m show_diff([idx_to_char[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m id_open\u001b[38;5;241m.\u001b[39mtolist()], [\u001b[43midx_to_char\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m id_close\u001b[38;5;241m.\u001b[39mtolist()])\n",
      "\u001b[0;31mKeyError\u001b[0m: -3"
     ]
    }
   ],
   "source": [
    "compute_diff(model.generate(max_len=64).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualize embeddings\n",
    "with torch.no_grad():\n",
    "    emb = model.tok_emb.weight.cpu().numpy()\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(emb, cmap=\"viridis\")\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Embeddings\")\n",
    "    plt.xlabel(\"Embedding dimension\")\n",
    "    plt.ylabel(\"Vocabulary index\")\n",
    "    plt.yticks(list(idx_to_char.keys()), list(idx_to_char.values()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
