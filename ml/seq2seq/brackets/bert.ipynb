{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 64])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"brackets.txt\", \"r\") as f:\n",
    "    brackets = f.readlines()\n",
    "idxs = list(map(lambda line: list(map(lambda val: int(val), line.split())), brackets))\n",
    "idxs = torch.tensor(idxs, dtype=torch.long)\n",
    "idxs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = 0.8\n",
    "train_size = int(train_val_split * len(idxs))\n",
    "\n",
    "train_idxs = idxs[:train_size]\n",
    "val_idxs = idxs[train_size:]\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_idxs)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_idxs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[<{[[<{{[([[[[<{<<<[({(<<(((<<{{}}>>)))>>)})]>>>}>]]]])]}}>]]}>]'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_char = {\n",
    "    0: \"{\",\n",
    "    1: \"(\",\n",
    "    2: \"[\",\n",
    "    3: \"<\",\n",
    "    4: \"}\",\n",
    "    5: \")\",\n",
    "    6: \"]\",\n",
    "    7: \">\",\n",
    "    8: \" MASK \",\n",
    "    9: \"_\"\n",
    "}\n",
    "char_to_idx = {v: k for k, v in idx_to_char.items()}\n",
    "def decode_brackets(brackets):\n",
    "    brackets = brackets.tolist()\n",
    "    return \"\".join([idx_to_char[idx] for idx in brackets])\n",
    "\n",
    "decode_brackets(idxs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<{[[<{{[([[[[<{<<<[({(<<(((<<{{}}>>)))>>)})]>>>}>]]]])]}}>]]}>\n",
      "<{[[<{{[([[[[<{<<<[({(<<(((<<{{}}>>)))>>)})]>>>}>]]]])]}}>]]}>"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "def show_diff(seq1, seq2):\n",
    "    diff = difflib.ndiff(seq1, seq2)\n",
    "    diff = list(diff)\n",
    "    print(\"\".join(seq1), end=\"\")\n",
    "    diff = reversed(diff)\n",
    "    for d in diff:\n",
    "        if d[0] == \"-\":\n",
    "            idx = char_to_idx[d[1:].strip()] + 4\n",
    "            print(f\"\\033[91m{idx_to_char[idx]}\\033[0m\", end=\"\")\n",
    "        elif d[0] == \"+\":\n",
    "            idx = char_to_idx[d[1:].strip()] + 4\n",
    "            print(f\"\\033[92m{idx_to_char[idx]}\\033[0m\", end=\"\")\n",
    "        else:\n",
    "            idx = char_to_idx[d.strip()] + 4\n",
    "            print(idx_to_char[idx], end=\"\")\n",
    "\n",
    "def compute_diff(seq):\n",
    "    seq = decode_brackets(seq).strip(\"_\").strip(\"[SOS] \").strip(\" [EOS]\")\n",
    "    print(seq)\n",
    "    brack_open = seq[:len(seq) // 2]\n",
    "    id_open = torch.tensor([char_to_idx[char] for char in brack_open], dtype=torch.long)\n",
    "    id_close = reversed(torch.tensor([char_to_idx[char] for char in seq[id_open.shape[0]:]], dtype=torch.long)) - 4\n",
    "    show_diff([idx_to_char[idx] for idx in id_open.tolist()], [idx_to_char[idx] for idx in id_close.tolist()])\n",
    "\n",
    "\n",
    "compute_diff(idxs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for inputs, in data_loader:\n",
    "        x = inputs.to(device)\n",
    "\n",
    "        # mask input\n",
    "        y = x.clone()\n",
    "        idx = torch.randint(1, x.shape[1] - 1, (1,))\n",
    "        x[:, idx:] = 9\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        loss = F.cross_entropy(y_pred[:, :-1].reshape(-1, y_pred.shape[-1]), y[:, 1:].reshape(-1))\n",
    "        total_loss += loss\n",
    "       \n",
    "    return total_loss.item() / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:05<00:00, 11.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "}>>(()[<}{){>[[>(({[)<}<)[)<(<}>]){}<<>}<[}((}}<{>>])]][<<>><]<}>\n",
      "Evaluation loss: 2.0815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "Model                                              [1, 63, 8]                224\n",
       "├─Embedding: 1-1                                   [1, 63, 4]                32\n",
       "├─Sequential: 1-2                                  --                        --\n",
       "│    └─TransformerBlock: 2-1                       [1, 63, 4]                288\n",
       "│    └─TransformerBlock: 2-2                       [1, 63, 4]                288\n",
       "│    └─TransformerBlock: 2-3                       [1, 63, 4]                288\n",
       "│    └─TransformerBlock: 2-4                       [1, 63, 4]                288\n",
       "├─Linear: 1-3                                      [1, 63, 8]                32\n",
       "====================================================================================================\n",
       "Total params: 1,440\n",
       "Trainable params: 1,440\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "====================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.12\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.12\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, emb_dim, atten_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            SelfAttention(emb_dim, num_heads, atten_dropout) for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(emb_dim * num_heads, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        heads = [head(x) for head in self.heads]\n",
    "        x = torch.cat(heads, dim=-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, m=4):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            MultiHeadAttention(num_heads, emb_dim),\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim, m * emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(m * emb_dim, emb_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.fc(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab=8, emb_dim=4, seq_len=64):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.emb_dim = emb_dim\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab, emb_dim)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(seq_len, emb_dim))\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            TransformerBlock(emb_dim, num_heads=2, m=2),\n",
    "            TransformerBlock(emb_dim, num_heads=2, m=2),\n",
    "            TransformerBlock(emb_dim, num_heads=2, m=2),\n",
    "            TransformerBlock(emb_dim, num_heads=2, m=2),\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(emb_dim, vocab, bias=False)\n",
    "        \n",
    "        # init weights\n",
    "        self.lm_head.weight = self.tok_emb.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[1] > self.seq_len:\n",
    "            x = x[:, -self.seq_len:]\n",
    "        \n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb[:tok_emb.shape[1]]\n",
    "        \n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, start: list[int] | torch.Tensor | None = None, max_len: int = 100, temperature: float = 1.0, top_k: int = 0, use_cache: bool = False):\n",
    "        if start is None:\n",
    "            start = torch.randint(self.vocab, (1, 1), device=device)\n",
    "        elif isinstance(start, list):\n",
    "            start = torch.tensor(start, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        if use_cache:\n",
    "            self.toggle_kv_cache(True)\n",
    "\n",
    "        x = start\n",
    "\n",
    "        for _ in tqdm(range(max_len)):\n",
    "            y_pred = self(x)\n",
    "            y_pred = y_pred[:, -1, :] / temperature\n",
    "            if top_k > 0:\n",
    "                y_pred = torch.topk(y_pred, top_k, dim=-1).values\n",
    "            next_char = torch.multinomial(torch.nn.functional.softmax(y_pred, dim=-1), 1)\n",
    "            x = torch.cat([x, next_char], dim=1)\n",
    "        \n",
    "        self.toggle_kv_cache(False)\n",
    "        return x\n",
    "    \n",
    "    def toggle_kv_cache(self, value: bool):\n",
    "        if value:\n",
    "            self.blocks.apply(lambda module: setattr(module, \"kv_cache\", {\"k\": torch.empty(0), \"v\": torch.empty(0)}))\n",
    "        else:\n",
    "            self.blocks.apply(lambda module: setattr(module, \"kv_cache\", None))\n",
    "\n",
    "model = Model().to(device)\n",
    "# model.toggle_kv_cache(True)\n",
    "print(decode_brackets(model.generate(max_len=64, use_cache=False).squeeze()))\n",
    "print(f\"Evaluation loss: {evaluate(model, val_loader):.4f}\")\n",
    "summary(model, (1, 63), dtypes=[torch.long], device=device, depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  38%|███▊      | 71/188 [00:05<00:09, 12.25it/s, loss=1.43, val_loss=1.71]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m1\u001b[39m,))\n\u001b[1;32m     16\u001b[0m x[:, idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m---> 18\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     21\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(x)\n",
      "File \u001b[0;32m~/projects/playground/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1528\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1528\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if \"val_loss\" in locals():\n",
    "    pass\n",
    "else:\n",
    "    val_loss = float(\"inf\")\n",
    "\n",
    "def loss_fn(y_pred, y):\n",
    "    # masked loss\n",
    "    \n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for x, in pbar:\n",
    "        x = x.to(device)\n",
    "\n",
    "        # mask input\n",
    "        y = x.clone()\n",
    "        idx = torch.randint(1, x.shape[1] - 1, (1,))\n",
    "        x[:, idx] = 8\n",
    "        \n",
    "        y_pred = model(x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = F.cross_entropy(y_pred[:, :-1].reshape(-1, y_pred.shape[-1]), y[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item(), val_loss=val_loss)\n",
    "\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    print(f\"Validation loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{{<(({[{(<<({{< MASK {{[{{{<<[{<[[{{<>}}]]>}]>>}}}]}}]>}})>>)}]}))>}}'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_brackets(x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualize embeddings\n",
    "with torch.no_grad():\n",
    "    emb = model.tok_emb.weight.cpu().numpy()\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(emb, cmap=\"viridis\")\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Embeddings\")\n",
    "    plt.xlabel(\"Embedding dimension\")\n",
    "    plt.ylabel(\"Vocabulary index\")\n",
    "    plt.yticks(list(idx_to_char.keys()), list(idx_to_char.values()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
