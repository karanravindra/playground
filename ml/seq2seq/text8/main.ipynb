{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text8.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "vocab = \" abcdefghijklmnopqrstuvwxyz\"\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "\n",
    "# add unknown token as last token\n",
    "char_to_idx[\"<unk>\"] = len(char_to_idx)\n",
    "idx_to_char[len(idx_to_char)] = \"<unk>\"\n",
    "\n",
    "print(f\"Vocab size: {len(vocab):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = torch.tensor([char_to_idx[char] for char in text], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 128\n",
    "split = 0.2\n",
    "\n",
    "train_len = int((1 - split) * len(text))\n",
    "\n",
    "train_text = text[:train_len]\n",
    "val_text = text[train_len:]\n",
    "print(f\"Train size: {len(train_text):,}\")\n",
    "print(f\"Val size: {len(val_text):,}\")\n",
    "\n",
    "train_text = train_text[:len(train_text) // seq_len * seq_len].view(-1, seq_len)\n",
    "val_text = val_text[:len(val_text) // seq_len * seq_len].view(-1, seq_len)\n",
    "print(f\"Train size: {train_text.shape}\")\n",
    "print(f\"Val size: {val_text.shape}\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(train_text),\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    "\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(val_text),\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str) -> torch.Tensor:\n",
    "    text = text.lower()\n",
    "    return torch.tensor([char_to_idx[char] if char in char_to_idx else char_to_idx[\"<unk>\"]for char in text], dtype=torch.long)\n",
    "\n",
    "def decode(tensor: torch.Tensor) -> str:\n",
    "    tensor = tensor.tolist()\n",
    "    return \"\".join([idx_to_char[idx] for idx in tensor])\n",
    "\n",
    "encoded = encode(\"Hello, World!\")\n",
    "decoded = decode(encoded)\n",
    "print(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(data_loader, desc=\"Evaluating\", leave=False)\n",
    "    for i, (inputs,) in enumerate(pbar):\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        x = inputs[:, :-1]\n",
    "        y = inputs[:, 1:]\n",
    "\n",
    "        y_pred = model(x)\n",
    "        loss = model.loss(y_pred, y)\n",
    "        total_loss += loss\n",
    "\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "       \n",
    "    return total_loss.item() / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotary_position_embedding(dim, seq_len, base=10000):\n",
    "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "    positions = torch.arange(seq_len, dtype=torch.float)\n",
    "    sinusoid_inp = torch.einsum('i,j->ij', positions, inv_freq)\n",
    "    \n",
    "    sin_emb = torch.sin(sinusoid_inp)\n",
    "    cos_emb = torch.cos(sinusoid_inp)\n",
    "\n",
    "    rotary_emb = torch.stack((sin_emb, cos_emb), dim=-1).reshape(seq_len, dim)\n",
    "    return rotary_emb\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, rotary_emb):\n",
    "    seq_len = q.size(-2)\n",
    "    rotary_emb = rotary_emb[:seq_len, :]  # Trim to the sequence length if necessary\n",
    "    cos_pos = rotary_emb[..., 1::2].repeat_interleave(2, dim=-1)\n",
    "    sin_pos = rotary_emb[..., 0::2].repeat_interleave(2, dim=-1)\n",
    "\n",
    "    q_rot = (q * cos_pos) + (torch.roll(q, shifts=1, dims=-1) * sin_pos)\n",
    "    k_rot = (k * cos_pos) + (torch.roll(k, shifts=1, dims=-1) * sin_pos)\n",
    "\n",
    "    return q_rot, k_rot\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd: int, n_head: int, attn_dropout: float = 0.0, is_causal: bool = True):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0, f\"Embedding dimension {n_embd} should be divisible by number of heads {n_head}\"\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embd, n_embd * 3)\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.is_causal = is_causal\n",
    "        \n",
    "        self.rotary_emb = None\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        # Initialize rotary embedding if not already done\n",
    "        if self.rotary_emb is None or self.rotary_emb.size(0) < T:\n",
    "            self.rotary_emb = rotary_position_embedding(C // self.n_head, T, base=10000).to(x.device)\n",
    "        \n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "\n",
    "        # (B, T, C) -> (B, T, n_head, C // n_head) -> (B, n_head, T, C // n_head)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # Apply rotary position embedding\n",
    "        q, k = apply_rotary_pos_emb(q, k, self.rotary_emb)\n",
    "\n",
    "        y = (\n",
    "            F.scaled_dot_product_attention(\n",
    "                q, k, v, is_causal=self.is_causal, dropout_p=self.attn_dropout\n",
    "            )\n",
    "            .transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(B, T, C)\n",
    "        )\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, emb_dim, atten_dropout=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            SelfAttention(emb_dim, num_heads, atten_dropout) for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(emb_dim * num_heads, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        heads = [head(x) for head in self.heads]\n",
    "        x = torch.cat(heads, dim=-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, m=4):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            MultiHeadAttention(num_heads, emb_dim),\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim, m * emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(m * emb_dim, emb_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.fc(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(len(vocab), emb_dim)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(seq_len, emb_dim))\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            TransformerBlock(emb_dim, num_heads=4, m=4),\n",
    "            TransformerBlock(emb_dim, num_heads=4, m=4),\n",
    "            TransformerBlock(emb_dim, num_heads=4, m=4),\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(emb_dim, len(vocab), bias=False)\n",
    "        \n",
    "        # init weights\n",
    "        self.lm_head.weight = self.tok_emb.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[1] > seq_len:\n",
    "            x = x[:, -seq_len:]\n",
    "\n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb[:tok_emb.shape[1]]\n",
    "        \n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, y_pred, y):\n",
    "        return torch.nn.functional.cross_entropy(y_pred.reshape(-1, y_pred.shape[-1]), y.reshape(-1), ignore_index=-1)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, start: list[int] | torch.Tensor | None = None, max_len: int = 100, temperature: float = 1.0, top_k: int = 0):\n",
    "        if start is None:\n",
    "            start = torch.randint(len(vocab), (1, 1), device=device)\n",
    "        elif isinstance(start, list):\n",
    "            start = torch.tensor(start, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        x = start\n",
    "\n",
    "        for _ in tqdm(range(max_len)):\n",
    "            y_pred = self(x)\n",
    "            y_pred = y_pred[:, -1, :] / temperature\n",
    "            if top_k > 0:\n",
    "                y_pred = torch.topk(y_pred, top_k, dim=-1).values\n",
    "            next_char = torch.multinomial(torch.nn.functional.softmax(y_pred, dim=-1), 1)\n",
    "            x = torch.cat([x, next_char], dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = Model(32).to(device)\n",
    "print(decode(model.generate(max_len=128).squeeze()))\n",
    "# print(f\"Evaluation loss: {evaluate(model, val_loader):.4f}\")\n",
    "summary(model, (64, 64), dtypes=[torch.long], device=device, depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"val_loss\" in locals():\n",
    "    pass\n",
    "else:\n",
    "    val_loss = float(\"inf\")\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for inputs, in pbar:\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # for _ in range(10):\n",
    "        x = inputs[:, :-1]\n",
    "        y = inputs[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        y_pred = model(x)\n",
    "        loss = F.cross_entropy(y_pred.reshape(-1, y_pred.shape[-1]), y.reshape(-1), ignore_index=-1)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix(loss=loss.item(), val_loss=val_loss)\n",
    "\n",
    "    val_loss = evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(model.generate(max_len=128, top_k=len(vocab)).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
