{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "text = urllib.request.urlopen(\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\").read().decode(\"utf-8\")\n",
    "\n",
    "vocab = sorted(set(text))\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "text = torch.tensor([char_to_idx[char] for char in text], dtype=torch.long)\n",
    "print(f\"Vocab size: {len(vocab):,}\")\n",
    "\n",
    "seq_len = 64\n",
    "split = 0.2\n",
    "\n",
    "train_len = int((1 - split) * len(text))\n",
    "\n",
    "train_text = text[:train_len]\n",
    "val_text = text[train_len:]\n",
    "print(f\"Train size: {len(train_text):,}\")\n",
    "print(f\"Val size: {len(val_text):,}\")\n",
    "\n",
    "train_text = train_text[:len(train_text) // seq_len * seq_len].view(-1, seq_len)\n",
    "val_text = val_text[:len(val_text) // seq_len * seq_len].view(-1, seq_len)\n",
    "print(f\"Train size: {train_text.shape}\")\n",
    "print(f\"Val size: {val_text.shape}\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(train_text),\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    "\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(val_text),\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str) -> torch.Tensor:\n",
    "    return torch.tensor([char_to_idx[char] for char in text], dtype=torch.long)\n",
    "\n",
    "def decode(tensor: torch.Tensor) -> str:\n",
    "    tensor = tensor.tolist()\n",
    "    return \"\".join([idx_to_char[idx] for idx in tensor])\n",
    "\n",
    "encoded = encode(\"Hello, World!\")\n",
    "decoded = decode(encoded)\n",
    "assert decoded == \"Hello, World!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for inputs, in data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        x = inputs[:, :-1]\n",
    "        y = inputs[:, 1:]\n",
    "\n",
    "        y_pred = model(x)\n",
    "        loss = model.loss(y_pred, y)\n",
    "        total_loss += loss\n",
    "       \n",
    "    return total_loss.item() / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_zoo.models.components import SelfAttention\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, emb_dim, atten_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            SelfAttention(emb_dim, num_heads, atten_dropout) for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(emb_dim * num_heads, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        heads = [head(x) for head in self.heads]\n",
    "        x = torch.cat(heads, dim=-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, m=4):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            MultiHeadAttention(num_heads, emb_dim),\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim, m * emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(m * emb_dim, emb_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.fc(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(len(vocab), emb_dim)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(seq_len, emb_dim))\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            TransformerBlock(emb_dim, num_heads=4, m=4),\n",
    "            TransformerBlock(emb_dim, num_heads=4, m=4),\n",
    "            TransformerBlock(emb_dim, num_heads=4, m=4),\n",
    "            TransformerBlock(emb_dim, num_heads=4, m=4),\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(emb_dim, len(vocab), bias=False)\n",
    "        \n",
    "        # init weights\n",
    "        self.lm_head.weight = self.tok_emb.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[1] > seq_len:\n",
    "            x = x[:, -seq_len:]\n",
    "\n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb[:tok_emb.shape[1]]\n",
    "        \n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, y_pred, y):\n",
    "        return torch.nn.functional.cross_entropy(y_pred.reshape(-1, y_pred.shape[-1]), y.reshape(-1), ignore_index=-1)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, start: list[int] | torch.Tensor | None = None, max_len: int = 100, temperature: float = 1.0, top_k: int = 0):\n",
    "        if start is None:\n",
    "            start = torch.randint(len(vocab), (1, 1), device=device)\n",
    "        elif isinstance(start, list):\n",
    "            start = torch.tensor(start, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        x = start\n",
    "\n",
    "        for _ in tqdm(range(max_len)):\n",
    "            y_pred = self(x)\n",
    "            y_pred = y_pred[:, -1, :] / temperature\n",
    "            if top_k > 0:\n",
    "                y_pred = torch.topk(y_pred, top_k, dim=-1).values\n",
    "            next_char = torch.multinomial(torch.nn.functional.softmax(y_pred, dim=-1), 1)\n",
    "            x = torch.cat([x, next_char], dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = Model(64).to(device)\n",
    "# print(decode(model.generate(max_len=128).squeeze()))\n",
    "print(f\"Evaluation loss: {evaluate(model, val_loader):.4f}\")\n",
    "summary(model, (64, 63), dtypes=[torch.long], device=device, depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"val_loss\" in locals():\n",
    "    pass\n",
    "else:\n",
    "    val_loss = float(\"inf\")\n",
    "\n",
    "pbar = tqdm(range(1000))\n",
    "for epoch in pbar:\n",
    "    model.train()\n",
    "    \n",
    "    for inputs, in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # for _ in range(10):\n",
    "        x = inputs[:, :-1]\n",
    "        y = inputs[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        y_pred = model(x)\n",
    "        loss = model.loss(y_pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix(loss=loss.item(), val_loss=val_loss)\n",
    "\n",
    "    val_loss = evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(model.generate(max_len=128).squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for named parameters, print gradient statistics\n",
    "print(\"Gradient statistics\")\n",
    "print(f\"{'Name':30} {'Mean':<7} {'Std':<7} {'Norm':<7}\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:30} {param.grad.mean():.5f} {param.grad.std():.5f} {param.grad.norm():.5f}\")\n",
    "\n",
    "    # clip gradient norm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def clamp_gradients(model, max_value: float):\n",
    "    for param in model.parameters():\n",
    "        param.grad.clamp_(-max_value, max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clamp_gradients(model, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
