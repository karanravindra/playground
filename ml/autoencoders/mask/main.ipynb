{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "from torchmetrics.functional.image.ssim import structural_similarity_index_measure as ssim_func\n",
    "from nn_zoo.datamodules import MNISTDataModule\n",
    "from nn_zoo.models.components import DepthwiseSeparableConv2d, SelfAttention\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomMask:\n",
    "    def __init__(self, patch_size, mask_prob=0.5, mask_value=0):\n",
    "        self.patch_size = (patch_size, patch_size)\n",
    "        self.mask_prob = mask_prob\n",
    "        self.mask_value = mask_value\n",
    "\n",
    "    def __call__(self, x):\n",
    "        batch_size, _, height, width = x.shape\n",
    "        \n",
    "        # Generate random start positions for each image in the batch\n",
    "        start_pos = torch.randint(\n",
    "            low=0, \n",
    "            high=min(height, width) - self.patch_size[0], \n",
    "            size=(batch_size, 2), \n",
    "            device=x.device\n",
    "        )\n",
    "\n",
    "        # Create a mask tensor initialized to ones\n",
    "        mask = torch.ones_like(x)\n",
    "        \n",
    "        # Calculate the indices to apply the mask\n",
    "        batch_indices = torch.arange(batch_size, device=x.device).view(batch_size, 1, 1)\n",
    "        h_indices = torch.arange(self.patch_size[0], device=x.device).view(1, self.patch_size[0], 1)\n",
    "        w_indices = torch.arange(self.patch_size[1], device=x.device).view(1, 1, self.patch_size[1])\n",
    "        \n",
    "        # Apply the mask in a vectorized manner\n",
    "        mask[batch_indices, :, \n",
    "             start_pos[:, 0].view(-1, 1, 1) + h_indices, \n",
    "             start_pos[:, 1].view(-1, 1, 1) + w_indices] = self.mask_value\n",
    "        \n",
    "        # Calculate the number of images to leave unmasked\n",
    "        num_to_unmask = int((1 - self.mask_prob) * batch_size)\n",
    "\n",
    "        if num_to_unmask > 0:\n",
    "            # Set the first num_to_unmask images to be fully unmasked (mask = 1)\n",
    "            mask[:num_to_unmask, :, :, :] = 1\n",
    "\n",
    "        return x * mask\n",
    "\n",
    "\n",
    "dm = MNISTDataModule(\n",
    "        data_dir=\"../../../data\",\n",
    "        dataset_params={\n",
    "            \"download\": True,\n",
    "            \"transform\": torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.Resize((32, 32)),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.RandomRotation(30)\n",
    "                ]\n",
    "            )\n",
    "        },\n",
    "        loader_params={\n",
    "            \"batch_size\": 128,\n",
    "        },\n",
    "    )\n",
    "\n",
    "dm.setup()\n",
    "\n",
    "train_loader = dm.train_dataloader()\n",
    "val_loader = dm.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(val_loader))\n",
    "x = RandomMask(16, 0.5, 0)(x)\n",
    "grid = torchvision.utils.make_grid(x[:], nrow=8)\n",
    "plt.imshow(grid.permute(1, 2, 0))\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, num_layers: int):\n",
    "        super(Block, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                self._block(in_channels, out_channels)\n",
    "                if i == 0\n",
    "                else self._block(out_channels, out_channels)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels: int, out_channels: int):\n",
    "        return nn.Sequential(\n",
    "            nn.GroupNorm(1, in_channels),\n",
    "            nn.GELU(),\n",
    "            DepthwiseSeparableConv2d(in_channels, out_channels, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers[0](x)\n",
    "        for i, layer in enumerate(self.layers[1:]):\n",
    "            x = layer(x) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels: int, out_channels: int, depth: int):\n",
    "        super(DownBlock, self).__init__(\n",
    "            Block(in_channels * 4, out_channels, depth),\n",
    "            # nn.MaxPool2d(2)\n",
    "            nn.PixelUnshuffle(2),\n",
    "        )\n",
    "\n",
    "\n",
    "class UpBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels: int, out_channels: int, depth: int):\n",
    "        super(UpBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.PixelShuffle(2),\n",
    "            # nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            Block(in_channels, out_channels * 4, depth),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, width: int, depth: int):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            Block(1, width * 4, depth),\n",
    "            DownBlock(width, width, depth),\n",
    "            DownBlock(width, width, depth),\n",
    "            DownBlock(width, width, depth),\n",
    "            DepthwiseSeparableConv2d(width * 4, width * 2, 1, padding=0),\n",
    "        )\n",
    "        self.proj_in = nn.Identity()\n",
    "        self.vq = nn.Identity()\n",
    "        # VectorQuantizer(width, 8, use_ema=True, decay=0.99, epsilon=1e-5)\n",
    "        self.proj_out = nn.Identity()  # nn.Conv2d(width, width, 1)\n",
    "        self.decoder = nn.Sequential(\n",
    "            DepthwiseSeparableConv2d(width * 2, width * 4, 1, padding=0),\n",
    "            UpBlock(width, width, depth),\n",
    "            UpBlock(width, width, depth),\n",
    "            UpBlock(width, width, depth),\n",
    "            Block(width * 4, 1, depth),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        # self.register_module(\n",
    "        #     \"lpips\", lpips.LPIPS(net=\"squeeze\", verbose=False, lpips=False)\n",
    "        # )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.proj_in(x)\n",
    "        return self.vq(x)  # quant_x, dict_loss, commit_loss, indices = self.vq(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        x = self.proj_out(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        x = self.decode(x)\n",
    "        return x\n",
    "\n",
    "    # @classmethod\n",
    "    def loss(self, x, y):\n",
    "        mse = F.mse_loss(x, y)\n",
    "        bce = F.binary_cross_entropy((x + 1) / 2, (y + 1) / 2)\n",
    "        psnr = 10 * (1 / mse).log10()\n",
    "        ssim = ssim_func(x, y)\n",
    "        # lpips = self.lpips(x.repeat(1, 3, 1, 1), y.repeat(1, 3, 1, 1)).mean()\n",
    "\n",
    "        return {\n",
    "            \"loss\": bce, # + lpips,\n",
    "            \"bce\": bce,\n",
    "            \"mse\": mse,\n",
    "            \"ssim\": ssim,\n",
    "            \"psnr\": psnr,\n",
    "            # \"lpips\": lpips,\n",
    "        }\n",
    "    \n",
    "model = AutoEncoder(4, 4)\n",
    "summary(model, input_size=(1, 1, 32, 32), depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    val_ssim = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    for x, y in tqdm(val_loader, desc=\"val\"):\n",
    "        x = x.to(\"mps\")\n",
    "        y_hat = model(x)\n",
    "        loss = F.binary_cross_entropy(y_hat, x) + lpips.lpips(y_hat, x)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        val_ssim += ssim_func(y_hat, x).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_ssim /= len(val_loader)\n",
    "\n",
    "    return val_loss, val_ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = DepthwiseSeparableConv2d(1, 6, 3, 1, 1)\n",
    "        self.conv2 = DepthwiseSeparableConv2d(6, 16, 3, 1, 1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16 * 8 * 8, 10),\n",
    "        )\n",
    "\n",
    "        self.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "    def forward(self, x, lpips=False):\n",
    "        x1 = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x2 = F.max_pool2d(F.relu(self.conv2(x1)), 2)\n",
    "\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "        x = self.fc(x2)\n",
    "\n",
    "        if lpips:\n",
    "            return x, x1, x2\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def lpips(self, img1, img2, linear=False):\n",
    "        # Forward pass with lpips=True to get feature maps\n",
    "        x_features1, x1_features1, x2_features1 = self.forward(img1, lpips=True)\n",
    "        x_features2, x1_features2, x2_features2 = self.forward(img2, lpips=True)\n",
    "\n",
    "        # Compute L2 distances between corresponding feature maps\n",
    "        dist1 = F.mse_loss(x1_features1, x1_features2)\n",
    "        dist2 = F.mse_loss(x2_features1, x2_features2)\n",
    "\n",
    "        # Sum the distances to get the LPIPS score\n",
    "        lpips_score = dist1 + dist2\n",
    "\n",
    "        # if linear:\n",
    "        #     lpips_score += F.mse_loss(x_features1, x_features2)\n",
    "\n",
    "        return lpips_score\n",
    "    \n",
    "lpips = Model().to(\"mps\")\n",
    "lpips.eval()\n",
    "\n",
    "print(lpips.lpips(torch.ones((1, 1, 32, 32), device=\"mps\"), torch.zeros((1, 1, 32, 32), device=\"mps\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "model.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'val_loss' not in locals() or 'val_ssim' not in locals():\n",
    "    val_loss, val_ssim = evaluate()\n",
    "\n",
    "for epoch in range(10):\n",
    "    masker = RandomMask(8, 0)\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\", ncols=150)\n",
    "\n",
    "    val_loss, val_ssim = evaluate()\n",
    "    \n",
    "    for x, y in pbar:\n",
    "        x = x.to(\"mps\")\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x)\n",
    "        loss = F.binary_cross_entropy(y_hat, x) + lpips.lpips(y_hat, x, linear=False)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix_str(f\"train_loss: {loss.item():.4f}, val_loss: {val_loss:.4f}, val_ssim: {val_ssim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot reconstruction\n",
    "x, y = next(iter(val_loader))\n",
    "x = x.to(\"mps\")\n",
    "\n",
    "model.eval()\n",
    "y_hat = model(x)\n",
    "grid = torchvision.utils.make_grid(y_hat, nrow=8)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(grid.permute(1, 2, 0).detach().cpu().numpy())\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for x, y in tqdm(train_loader):\n",
    "        x, y =  x.to(\"mps\"), y.to(\"mps\")\n",
    "        y_hat = model(x)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
