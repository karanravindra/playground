{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from ema_pytorch import EMA\n",
    "from torchinfo import summary\n",
    "from ml_zoo.datamodules import CIFARDataModule\n",
    "from ml_zoo.models.components import VectorQuantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = CIFARDataModule(\n",
    "    data_dir=\"data\",\n",
    "    dataset_params={\n",
    "        \"download\": True,\n",
    "        \"transform\": torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.Resize((32, 32)),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "            ]\n",
    "        ),\n",
    "    },\n",
    "    loader_params={\n",
    "        \"batch_size\": 64,\n",
    "    },\n",
    ")\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "trian_loader = dm.train_dataloader()\n",
    "test_loader = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(DepthwiseSeparableConv2d, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size, stride, padding, groups=in_channels\n",
    "        )\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.norm1 = nn.GroupNorm(4, out_channels)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        x = self.pointwise(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvStack(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        num_layers,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        conv_type=DepthwiseSeparableConv2d,\n",
    "    ):\n",
    "        super(ConvStack, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                conv_type(in_channels, out_channels, kernel_size, stride, padding)\n",
    "                if _ == 0\n",
    "                else conv_type(out_channels, out_channels, kernel_size, stride, padding)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers[0](x)\n",
    "        \n",
    "        for layer in self.layers[1:]:\n",
    "            residual = x\n",
    "            x = layer(x) + residual\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 4, 1),\n",
    "            # nn.PixelUnshuffle(2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ConvStack(16, 16, 4),\n",
    "            # nn.PixelUnshuffle(2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ConvStack(64, 64, 4),\n",
    "            # nn.PixelUnshuffle(2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ConvStack(256, 256, 1),\n",
    "        )\n",
    "\n",
    "        self.quantizer = VectorQuantizer(\n",
    "            embedding_dim=64,\n",
    "            num_embeddings=512,\n",
    "            use_ema=True,\n",
    "            decay=0.99,\n",
    "            epsilon=1e-5,\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            # nn.Conv2d(256, 256, 1),\n",
    "            # nn.PixelShuffle(2),\n",
    "            ConvStack(64, 64, 4),\n",
    "            nn.PixelShuffle(2),\n",
    "            ConvStack(16, 16, 4),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.Conv2d(4, 3, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        # print(x.shape)\n",
    "        (z_emb, dict_loss, commit_loss, idxs) = self.quantizer(x)\n",
    "        # print(z_emb.shape)\n",
    "        x = self.decoder(z_emb)\n",
    "        return (x, z_emb, dict_loss, commit_loss, idxs)\n",
    "\n",
    "\n",
    "model = Autoencoder().to(\"mps\")\n",
    "summary(\n",
    "    model,\n",
    "    input_data=torch.randn(1, 3, 32, 32, device=\"mps\", requires_grad=False),\n",
    "    depth=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ema = EMA(model, beta=0.9999, update_after_step=100, update_every=10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    pbar = tqdm(trian_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for img, _ in pbar:\n",
    "        img = img.to(\"mps\")\n",
    "        optimizer.zero_grad()\n",
    "        output = model(img)\n",
    "        \n",
    "        recon_error = criterion(output[0], img)\n",
    "        loss = recon_error\n",
    "        if output[2] is not None:\n",
    "            loss += output[2]\n",
    "        if output[3] is not None:\n",
    "            loss += output[3]\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ema.update()\n",
    "        pbar.set_postfix({\"loss\": loss.item(), \"test_loss\": test_loss})\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for img, _ in tqdm(test_loader, desc=\"Testing\", leave=True):\n",
    "            img = img.to(\"mps\")\n",
    "            output = model(img)\n",
    "            loss = criterion(output[0], img)\n",
    "            if output[2] is not None:\n",
    "                loss += output[2]\n",
    "            if output[3] is not None:\n",
    "                loss += output[3]\n",
    "            test_loss += loss\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_loss = test_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def plot_reconstructions(model):\n",
    "    model.eval()\n",
    "    img, _ = next(iter(test_loader))\n",
    "    img = img.to(\"mps\")\n",
    "    output = model(img)\n",
    "    display(\n",
    "        torchvision.transforms.ToPILImage()(\n",
    "            torchvision.utils.make_grid(\n",
    "                torch.cat([img, output[0]], dim=0).to(\"cpu\"), nrow=8\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "plot_reconstructions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "@torch.no_grad()\n",
    "def codebook_usage(model, loader):\n",
    "    model.eval()\n",
    "    counter = Counter()\n",
    "    for img, _ in tqdm(loader, desc=\"Calculating codebook usage\"):\n",
    "        img = img.to(\"mps\")\n",
    "        output = model(img)\n",
    "        idxs = output[4]\n",
    "        counter.update(idxs.cpu().numpy().flatten())\n",
    "    return counter\n",
    "\n",
    "# plot codebook usage\n",
    "usasge = codebook_usage(model, test_loader)\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.imshow(\n",
    "    torch.tensor([usasge[i] for i in range(512)]).reshape(16, 32).numpy(),\n",
    "    cmap=\"hot\",\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(usasge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
