{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baby Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "def extract_names() -> list[str]:\n",
    "    fURL = \"https://www.cs.cmu.edu/Groups/AI/util/areas/nlp/corpora/names/female.txt\"\n",
    "    mURL = \"https://www.cs.cmu.edu/Groups/AI/util/areas/nlp/corpora/names/male.txt\"\n",
    "\n",
    "    # Read text files\n",
    "    f = pd.read_csv(fURL, header=None)[5:]\n",
    "    m = pd.read_csv(mURL, header=None)[5:]\n",
    "\n",
    "    # Concatenate and return names\n",
    "    return f[0].tolist() + m[0].tolist()\n",
    "\n",
    "\n",
    "def save_train_test_split(path: str, ratio: float) -> None:\n",
    "    names = list(set(extract_names()))\n",
    "\n",
    "    random.shuffle(names)\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    train_split = sorted(names[: int(len(names) * ratio)])\n",
    "    test_split = sorted(names[int(len(names) * ratio) :])\n",
    "\n",
    "    with open(path + \"/train.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(train_split).encode(\"ascii\", errors=\"ignore\").decode(\"ascii\").lower())\n",
    "\n",
    "    with open(path + \"/test.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(test_split).encode(\"ascii\", errors=\"ignore\").decode(\"ascii\").lower())\n",
    "\n",
    "\n",
    "names = extract_names()\n",
    "save_train_test_split(\"data/names\", 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Train and test split\n",
    "with open(\"data/names/train.txt\") as f:\n",
    "    train_names = f.read().split(\"\\n\")\n",
    "\n",
    "with open(\"data/names/test.txt\") as f:\n",
    "    test_names = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Data Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Train: {len(train_names)}\")\n",
    "print(f\"Test: {len(test_names)}\")\n",
    "print(f\"Training names: {train_names[:5]}\")\n",
    "print(f\"Test names: {test_names[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(sorted(set(\"\".join(train_names))))\n",
    "print(f\"Vocab: {vocab}\")\n",
    "print(f\"Vocab size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Letter Pairings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the distribution of the names\n",
    "plt.figure(figsize=(30, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Distribution of name lengths\")\n",
    "plt.hist([len(name) + 2 for name in train_names], bins=20) # +2 for <s> and </s>\n",
    "plt.xlabel(\"Name length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Frequency per letter\")\n",
    "letter_freq = {letter: 0 for letter in vocab}\n",
    "for name in train_names:\n",
    "    for letter in name:\n",
    "        if letter.isalpha():\n",
    "            letter_freq[letter.lower()] += 1\n",
    "        \n",
    "plt.bar(letter_freq.keys(), letter_freq.values()) # type: ignore\n",
    "plt.xlabel(\"Letter\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Distribution of first letters\") # Ignoring <s>\n",
    "first_letter_freq = {letter: 0 for letter in vocab}\n",
    "for name in train_names:\n",
    "    first_letter_freq[name[0]] += 1\n",
    "plt.bar(first_letter_freq.keys(), first_letter_freq.values()) # type: ignore\n",
    "\n",
    "plt.xlabel(\"First letter\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confunsion matrix of letter pairs\n",
    "import numpy as np\n",
    "\n",
    "letter_pairs = [a + b for a in vocab for b in vocab]\n",
    "letter_pair_freq = {pair: 0 for pair in letter_pairs}\n",
    "for name in train_names:\n",
    "    for i in range(len(name) - 1):\n",
    "        letter_pair_freq[name[i:i+2].lower()] += 1\n",
    "\n",
    "confusion_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "for i, a in enumerate(vocab):\n",
    "    for j, b in enumerate(vocab):\n",
    "        confusion_matrix[i, j] = letter_pair_freq[a + b]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(confusion_matrix)\n",
    "plt.xticks(range(len(vocab)), vocab)\n",
    "plt.yticks(range(len(vocab)), vocab)\n",
    "plt.xlabel(\"Second letter\")\n",
    "plt.ylabel(\"First letter\")\n",
    "plt.title(\"Confusion matrix of letter pairs\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoded Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add start and end tokens\n",
    "special_tokens = [\"<s>\", \"</s>\", \"<pad>\", \"<unk>\"]\n",
    "vocab = special_tokens + vocab\n",
    "print(f\"Vocab: {vocab}\")\n",
    "print(f\"Vocab size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = max(len(name) for name in train_names) + 2\n",
    "print(f\"Max sequence length: {seq_len}\")\n",
    "\n",
    "\n",
    "def pad_sequence(sequence: list[int], seq_len: int) -> list[int]:\n",
    "    return sequence + [vocab.index(\"<pad>\")] * (seq_len - len(sequence))\n",
    "\n",
    "\n",
    "def encode_name(name: str) -> list[int]:\n",
    "    result = [vocab.index(\"<s>\")]\n",
    "    for letter in name.lower():\n",
    "        if letter in vocab:\n",
    "            result.append(vocab.index(letter))\n",
    "        else:\n",
    "            result.append(vocab.index(\"<unk>\"))\n",
    "    result.append(vocab.index(\"</s>\"))\n",
    "    return pad_sequence(result, seq_len)\n",
    "\n",
    "\n",
    "def decode_name(name: list[int]) -> str:\n",
    "    for token in special_tokens:\n",
    "        while vocab.index(token) in name:\n",
    "            name.remove(vocab.index(token))\n",
    "    \n",
    "\n",
    "    name = \"\".join([vocab[i] for i in name]) # type: ignore\n",
    "    return name[0].upper() + name[1:] # type: ignore\n",
    "\n",
    "\n",
    "print(encode_name(\"Alice\"))\n",
    "print(decode_name(encode_name(\"Alice\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded_names = [encode_name(name) for name in train_names]\n",
    "test_encoded_names = [encode_name(name) for name in test_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_encoded_names), len(test_encoded_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset and Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded_corpus = torch.Tensor(train_encoded_names).to(device)\n",
    "test_encoded_corpus = torch.Tensor(test_encoded_names).to(device)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_encoded_corpus)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_encoded_corpus)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for x, in test_loader:\n",
    "        inputs = x[:, :-1].long()\n",
    "        targets = x[:, 1:].long()\n",
    "\n",
    "        outputs, _ = model(inputs)\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(outputs.transpose(1, 2), targets)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    model.train()\n",
    "    return total_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, hidden_size, num_layers, dropout, batch_first=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.RNN(\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=batch_first,\n",
    "            dropout=dropout,\n",
    "            nonlinearity=\"relu\",\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, vocab_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hidden: torch.Tensor | None = None):\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(\n",
    "                self.rnn.num_layers, x.size(0), self.rnn.hidden_size, device=device\n",
    "            )\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x, hidden\n",
    "    \n",
    "    def forward_with_gradient(self, x, hidden=None):\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(self.rnn.num_layers, x.size(0), self.rnn.hidden_size, device=x.device)\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        output = self.mlp(x)\n",
    "        return output, hidden, x  # return embeddings for gradient computation\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        start_seq: str | None = None,\n",
    "        max_len: int = 20,\n",
    "        hidden: torch.Tensor | None = None,\n",
    "    ) -> str:\n",
    "        self.eval()\n",
    "        if start_seq is None:\n",
    "            start_seq = \"<s>\"\n",
    "\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(\n",
    "                self.rnn.num_layers, 1, self.rnn.hidden_size, device=device\n",
    "            )\n",
    "            \n",
    "        x = torch.Tensor([vocab.index(start_seq)]).long().unsqueeze(0).to(device)\n",
    "\n",
    "        output = [x.flatten()]\n",
    "        for _ in range(max_len):\n",
    "            x, hidden = self(x, hidden)\n",
    "            \n",
    "            if x.shape[1] > 1:\n",
    "                x = x[:, -1:]\n",
    "\n",
    "            x = x.softmax(dim=-1).argmax(dim=-1)\n",
    "            \n",
    "            if x.item() == vocab.index(\"</s>\") and len(output) > 2:\n",
    "                break\n",
    "            output.append(x.flatten())\n",
    "        self.train()\n",
    "        \n",
    "        return decode_name(torch.cat(output).flatten().tolist())\n",
    "\n",
    "\n",
    "hidden_size = 32\n",
    "num_layers = 12\n",
    "dropout = 0\n",
    "\n",
    "model = RNN(len(vocab), hidden_size, num_layers, dropout).to(device)\n",
    "num_train_steps = 0\n",
    "\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=4e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "roll_loss = 0\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    pbar = tqdm(train_loader, leave=True, desc=f\"Epoch {epoch + 1:02d}\")\n",
    "    val_loss = validate(model)\n",
    "\n",
    "    for batch in pbar:\n",
    "        batch = batch[0].to(device)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = batch[:, :-1].long()\n",
    "        targets = batch[:, 1:].long()\n",
    "\n",
    "        outputs, _ = model(inputs)\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(outputs.transpose(1, 2), targets)\n",
    "\n",
    "        roll_loss = 0.9 * roll_loss + 0.1 * loss.detach()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        num_train_steps += 1\n",
    "        \n",
    "\n",
    "        pbar.set_postfix_str(f\"Loss: {loss.item():.4f}, Val loss: {val_loss:.4f}, Steps: {num_train_steps}, LR: {optimizer.param_groups[0]['lr']:.2e}, Roll loss: {roll_loss:.4f}\")\n",
    "        pbar.update()\n",
    "\n",
    "    scheduler.step(roll_loss)\n",
    "\n",
    "model.eval()\n",
    "print(\"\\n\\nGenerated names:\")\n",
    "model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print(model.generate(hidden=torch.randn(num_layers, 1, hidden_size, device=device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"blog/4-names/models/rnn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, dropout, batch_first=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers,dropout=dropout, batch_first=batch_first)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden: tuple[torch.Tensor, torch.Tensor] | None = None):\n",
    "        if hidden is None:\n",
    "            hidden = (\n",
    "                torch.zeros(self.rnn.num_layers, x.size(0), self.rnn.hidden_size, device=x.device),\n",
    "                torch.zeros(self.rnn.num_layers, x.size(0), self.rnn.hidden_size, device=x.device),\n",
    "            )\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x, hidden\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        start_seq: str | None = None,\n",
    "        max_len: int = 20,\n",
    "        hidden: tuple[torch.Tensor, torch.Tensor] | None = None,\n",
    "    ) -> str:\n",
    "        self.eval()\n",
    "        if start_seq is None:\n",
    "            start_seq = \"<s>\"\n",
    "\n",
    "        if hidden is None:\n",
    "            hidden = (\n",
    "                torch.zeros(self.rnn.num_layers, 1, self.rnn.hidden_size, device=device),\n",
    "                torch.zeros(self.rnn.num_layers, 1, self.rnn.hidden_size, device=device)\n",
    "            )\n",
    "        x = torch.Tensor([vocab.index(start_seq)]).long().unsqueeze(0).to(device)\n",
    "\n",
    "        output = [x.flatten()]\n",
    "        for _ in range(max_len):\n",
    "            x, hidden = self(x, hidden)\n",
    "            if x.shape[1] > 1:\n",
    "                x = x[:, -1:]\n",
    "\n",
    "            x = x.softmax(dim=-1).argmax(dim=-1)\n",
    "            if x.item() == vocab.index(\"</s>\") and len(output) > 2:\n",
    "                break\n",
    "            output.append(x.flatten())\n",
    "        self.train()\n",
    "        \n",
    "        return decode_name(torch.cat(output).flatten().tolist())\n",
    "\n",
    "\n",
    "hidden_size = 32\n",
    "num_layers = 8\n",
    "dropout = 0.0\n",
    "\n",
    "model = LSTM(len(vocab), hidden_size, num_layers, dropout).to(device)\n",
    "num_train_steps = 0\n",
    "\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=4e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    pbar = tqdm(train_loader, leave=True, desc=f\"Epoch {epoch + 1:02d}\")\n",
    "    val_loss = validate(model)\n",
    "    \n",
    "    for batch in pbar:\n",
    "        batch = batch[0].to(device)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = batch[:, :-1].long()\n",
    "        targets = batch[:, 1:].long()\n",
    "\n",
    "        outputs, _ = model(inputs)\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(outputs.transpose(1, 2), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        num_train_steps += 1\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item(), val_loss=val_loss, step=num_train_steps)\n",
    "        pbar.update()\n",
    "\n",
    "model.eval()\n",
    "print(\"\\n\\nGenerated names:\")\n",
    "model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print(model.generate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"blog/4-names/models/lstm.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, hidden_size, num_layers, dropout, batch_first=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.gru = nn.GRU(\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=batch_first,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    def forward(self, x, hidden: torch.Tensor | None = None):\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(\n",
    "                self.gru.num_layers, x.size(0), self.gru.hidden_size, device=device\n",
    "            )\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.gru(x, hidden)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x, hidden\n",
    "    \n",
    "    def forward_with_gradient(self, x, hidden=None):\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(self.gru.num_layers, x.size(0), self.gru.hidden_size, device=x.device)\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.gru(x, hidden)\n",
    "        output = self.fc(x)\n",
    "        return output, hidden, x  # return embeddings for gradient computation\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        start_seq: str | None = None,\n",
    "        max_len: int = 20,\n",
    "        hidden: torch.Tensor | None = None,\n",
    "    ) -> str:\n",
    "        self.eval()\n",
    "        if start_seq is None:\n",
    "            start_seq = \"<s>\"\n",
    "\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(\n",
    "                self.gru.num_layers, 1, self.rnn.hidden_size, device=device\n",
    "            )\n",
    "        x = torch.Tensor([vocab.index(start_seq)]).long().unsqueeze(0).to(device)\n",
    "\n",
    "        output = [x.flatten()]\n",
    "        for _ in range(max_len):\n",
    "            x, hidden = self(x, hidden)\n",
    "            if x.shape[1] > 1:\n",
    "                x = x[:, -1:]\n",
    "\n",
    "            x = x.softmax(dim=-1).argmax(dim=-1)\n",
    "            if x.item() == vocab.index(\"</s>\") and len(output) > 2:\n",
    "                break\n",
    "            output.append(x.flatten())\n",
    "        self.train()\n",
    "        \n",
    "        return decode_name(torch.cat(output).flatten().tolist())\n",
    "\n",
    "\n",
    "hidden_size = 16\n",
    "num_layers = 8\n",
    "dropout = 0.1\n",
    "\n",
    "model = GRU(len(vocab), hidden_size, num_layers, dropout).to(device)\n",
    "num_train_steps = 0\n",
    "\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    pbar = tqdm(train_loader, leave=True, desc=f\"Epoch {epoch + 1:02d}\")\n",
    "    val_loss = validate(model)\n",
    "    \n",
    "    for batch in pbar:\n",
    "        batch = batch[0].to(device)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = batch[:, :-1].long()\n",
    "        targets = batch[:, 1:].long()\n",
    "\n",
    "        outputs, _ = model(inputs)\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(outputs.transpose(1, 2), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        num_train_steps += 1\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item(), val_loss=val_loss, step=num_train_steps)\n",
    "        pbar.update()\n",
    "\n",
    "print(\"\\n\\nGenerated names:\")\n",
    "model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print(model.generate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"blog/4-names/models/gru.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
