{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    0: \"{\",\n",
    "    1: \"(\",\n",
    "    2: \"[\",\n",
    "    3: \"<\",\n",
    "    4: \"}\",\n",
    "    5: \")\",\n",
    "    6: \"]\",\n",
    "    7: \">\",\n",
    "    8: \"[SOS]\",\n",
    "    9: \"[EOS]\",\n",
    "    10: \"[PAD]\",\n",
    "}\n",
    "\n",
    "\n",
    "def gen_seq(len: int) -> tuple[str, list[int]]:\n",
    "    opens = random.choices([0, 1, 2, 3], k=len // 2)\n",
    "    closes = list(reversed(list(map(lambda x: x + 4, opens))))\n",
    "\n",
    "    seq = [8] + opens + closes + [9]\n",
    "    return \"\".join([vocab[x] for x in seq]), seq\n",
    "\n",
    "\n",
    "def validate_sequence(seq: list[int]) -> bool:\n",
    "    stack = []\n",
    "    for x in seq:\n",
    "        if x in [0, 1, 2, 3]:\n",
    "            stack.append(x)\n",
    "        else:\n",
    "            if len(stack) == 0:\n",
    "                return False\n",
    "            if stack[-1] != x - 4:\n",
    "                return False\n",
    "            stack.pop()\n",
    "    return len(stack) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = [\n",
    "    gen_seq(16)[1] for _ in range(64_000)\n",
    "]\n",
    "test_seq = [\n",
    "    gen_seq(16)[1] for _ in range(16_000)\n",
    "]\n",
    "train_seq = torch.as_tensor(train_seq).long()\n",
    "test_seq = torch.as_tensor(test_seq).long()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_seq, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_seq, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 500 parameters\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\"\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = len(vocab)\n",
    "    block_size: int = 16 + 2\n",
    "    emb_size: int = 4\n",
    "    heads: int = 4\n",
    "    num_layers: int = 1\n",
    "    attn_dropout: float = 0\n",
    "    ff_mult: int = 1\n",
    "    ff_dropout: float = 0\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: GPTConfig, layer_idx, head_idx, cache_enabled=False):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_idx = head_idx\n",
    "        self.cache_enabled = cache_enabled\n",
    "        self.cache = {}\n",
    "\n",
    "        self.q = nn.Linear(config.emb_size, config.emb_size)\n",
    "        self.k = nn.Linear(config.emb_size, config.emb_size)\n",
    "        self.v = nn.Linear(config.emb_size, config.emb_size)\n",
    "\n",
    "        self.out = nn.Linear(config.emb_size, config.emb_size)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.attn_dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, C = x.size()\n",
    "        cache_key = (B, T, C)  # Example key; adjust based on your caching strategy\n",
    "\n",
    "        if self.cache_enabled and cache_key in self.cache:\n",
    "            k, v = self.cache[cache_key]['k'], self.cache[cache_key]['v']\n",
    "        else:\n",
    "            k = (\n",
    "                self.k(x)\n",
    "                .view(B, T, self.config.heads, C // self.config.heads)\n",
    "                .transpose(1, 2)\n",
    "            )\n",
    "            v = (\n",
    "                self.v(x)\n",
    "                .view(B, T, self.config.heads, C // self.config.heads)\n",
    "                .transpose(1, 2)\n",
    "            )\n",
    "            if self.cache_enabled:\n",
    "                self.cache[cache_key] = {'k': k, 'v': v}\n",
    "\n",
    "        q = (\n",
    "            self.q(x)\n",
    "            .view(B, T, self.config.heads, C // self.config.heads)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) / ((C // self.config.heads) ** 0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        return self.out(x), attn\n",
    "\n",
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig, layer_idx):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.heads = nn.ModuleList([AttentionHead(config, layer_idx=layer_idx, head_idx=i) for i in range(config.heads)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # input and output are the same size\n",
    "        attns = []\n",
    "        for head in self.heads:\n",
    "            attn, _ = head(x, mask=mask)\n",
    "            attns.append(attn)\n",
    "\n",
    "        return torch.mean(torch.stack(attns), dim=0)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPTConfig, layer_idx):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(config.emb_size)\n",
    "        self.attn = MaskedMultiHeadAttention(config, layer_idx)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(config.emb_size)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(config.emb_size, config.ff_mult * config.emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.ff_mult * config.emb_size, config.emb_size),\n",
    "        )\n",
    "\n",
    "        if config.ff_dropout > 0:\n",
    "            self.ff_dropout = nn.Dropout(config.ff_dropout)\n",
    "\n",
    "        if config.attn_dropout > 0:\n",
    "            self.attn_dropout = nn.Dropout(config.attn_dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        identity = x\n",
    "        x = self.ln1(x)\n",
    "        x = self.attn(x, mask=mask)\n",
    "\n",
    "        if hasattr(self, \"attn_dropout\"):\n",
    "            x = self.attn_dropout(x)\n",
    "\n",
    "        x = x + identity\n",
    "\n",
    "        identity = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.ff(x)\n",
    "\n",
    "        if hasattr(self, \"ff_dropout\"):\n",
    "            x = self.ff_dropout(x)\n",
    "\n",
    "        return x + identity\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.emb_size)\n",
    "        self.pos_emb = nn.Embedding(config.block_size, config.emb_size)\n",
    "\n",
    "        self.blocks = nn.ModuleList([Block(config, layer_idx=i) for i in range(config.num_layers)])\n",
    "\n",
    "        self.ln = nn.LayerNorm(config.emb_size)\n",
    "        self.head = nn.Linear(config.emb_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie weights\n",
    "        self.head.weight = self.token_emb.weight\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, std=0.02)\n",
    "\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "        assert not T > self.config.block_size, \"Sequence length is longer than block size\"\n",
    "\n",
    "        emb = self.token_emb(x)\n",
    "        pe = self.pos_emb(torch.arange(T-1, -1, step=-1, device=device))\n",
    "\n",
    "        x = emb + pe\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask=torch.tril(torch.ones(T, T, device=device)).view(1, T, T))\n",
    "\n",
    "        x = self.ln(x)\n",
    "        return self.head(x)\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        # Input is a contiguous tensor\n",
    "        y = y.flatten()\n",
    "        y_pred = y_pred.view(-1, y_pred.size(-1))\n",
    "\n",
    "        return F.cross_entropy(y_pred, y)\n",
    "\n",
    "    def get_param_count(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, max_len: int = 128, temperature: float = 1.0):\n",
    "        self.eval()\n",
    "\n",
    "        generated = [8]\n",
    "        primer_t = torch.as_tensor(generated, device=device).unsqueeze(0)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            if primer_t.size(1) > self.config.block_size:\n",
    "                primer_t = primer_t[:, -self.config.block_size :]\n",
    "            out = self(primer_t)\n",
    "            out = out[:, -1, :] / temperature\n",
    "            out = torch.multinomial(F.softmax(out, dim=-1), num_samples=1)\n",
    "\n",
    "            gen = out.item()\n",
    "            if gen == 9:\n",
    "                break\n",
    "            generated.append(gen)\n",
    "\n",
    "            primer_t = torch.cat((primer_t, out), dim=1)\n",
    "\n",
    "        return \"\".join([vocab[x] for x in generated]), generated\n",
    "\n",
    "\n",
    "config = GPTConfig()\n",
    "model = GPT(config).to(device)\n",
    "num_train_steps = 0\n",
    "\n",
    "print(f\"Model has {model.get_param_count():,} parameters\")\n",
    "# print(model.generate(max_len=64)[0])\n",
    "\n",
    "del config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  97%|█████████▋| 487/500 [00:09<00:00, 51.80it/s, loss=1.24, val_loss=0]"
     ]
    }
   ],
   "source": [
    "val_loss = 0\n",
    "for epoch in range(20):\n",
    "    pbar = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for seq in pbar:\n",
    "        seq = seq.to(device)\n",
    "        inputs = seq[:, :-1]\n",
    "        targets = seq[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = F.cross_entropy(outputs.transpose(1, 2), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item(), val_loss=val_loss)\n",
    "\n",
    "    val_loss = 0\n",
    "    for seq in test_loader:\n",
    "        seq = seq.to(device)\n",
    "        inputs = seq[:, :-1]\n",
    "        targets = seq[:, 1:]\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = F.cross_entropy(outputs.transpose(1, 2), targets)\n",
    "        val_loss += loss.item()\n",
    "    val_loss /= len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(F.softmax(model.forward(inputs[0].unsqueeze(0))[0, -1], dim=-1).cpu().detach().numpy()/0.1)\n",
    "plt.xticks(range(len(vocab)), vocab.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some sequences\n",
    "for _ in range(10):\n",
    "    print(model.generate(max_len=64)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
