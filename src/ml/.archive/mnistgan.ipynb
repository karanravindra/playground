{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "from tokenizers import Tokenizer, models, decoders, trainers, tools, pre_tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "trainer = trainers.BpeTrainer(special_tokens=[\"[PAD]\", \"[SOS]\", \"[EOS]\", \"[MASK]\", \"[UNK]\"], vocab_size=8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train([\"blog/5-shakespeare/data/train.txt\"], trainer=trainer)\n",
    "print(f\"Vocab size: {tokenizer.get_vocab_size()}\")\n",
    "\n",
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"blog/5-shakespeare/data/train.txt\", \"r\") as f:\n",
    "    train_corpus = f.read()\n",
    "\n",
    "with open(\"blog/5-shakespeare/data/test.txt\", \"r\") as f:\n",
    "    test_corpus = f.read()\n",
    "\n",
    "train_encoded_corpus = tokenizer.encode(train_corpus).ids\n",
    "val_encoded_corpus = tokenizer.encode(test_corpus).ids\n",
    "\n",
    "del train_corpus, test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, corpus, seq_len):\n",
    "        self.corpus = corpus\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.as_tensor(self.corpus[idx:idx+self.seq_len]), torch.as_tensor(self.corpus[idx+1:idx+self.seq_len+1])\n",
    "    \n",
    "\n",
    "seq_len = 64\n",
    "train_dataset = Dataset(train_encoded_corpus, seq_len)\n",
    "val_dataset = Dataset(val_encoded_corpus, seq_len)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "del train_encoded_corpus, val_encoded_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If model already exists, delete it\n",
    "if \"model\" in locals():\n",
    "    del model\n",
    "if \"optimizer\" in locals():\n",
    "    del optimizer\n",
    "if \"scheduler\" in locals():\n",
    "    del scheduler\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = tokenizer.get_vocab_size()\n",
    "    block_size: int = seq_len\n",
    "    emb_size: int = 64\n",
    "    heads: int = 8\n",
    "    num_layers: int = 1\n",
    "    attn_dropout: float = 0\n",
    "    ff_mult: int = 1\n",
    "    ff_dropout: float = 0\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: GPTConfig, layer_idx, head_idx, cache_enabled=False):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_idx = head_idx\n",
    "        self.cache_enabled = cache_enabled\n",
    "        self.cache = {}\n",
    "\n",
    "        self.q = nn.Linear(config.emb_size, config.emb_size)\n",
    "        self.k = nn.Linear(config.emb_size, config.emb_size)\n",
    "        self.v = nn.Linear(config.emb_size, config.emb_size)\n",
    "\n",
    "        self.out = nn.Linear(config.emb_size, config.emb_size)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.attn_dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, C = x.size()\n",
    "        cache_key = (B, T, C)  # Example key; adjust based on your caching strategy\n",
    "\n",
    "        if self.cache_enabled and cache_key in self.cache:\n",
    "            k, v = self.cache[cache_key][\"k\"], self.cache[cache_key][\"v\"]\n",
    "        else:\n",
    "            k = (\n",
    "                self.k(x)\n",
    "                .view(B, T, self.config.heads, C // self.config.heads)\n",
    "                .transpose(1, 2)\n",
    "            )\n",
    "            v = (\n",
    "                self.v(x)\n",
    "                .view(B, T, self.config.heads, C // self.config.heads)\n",
    "                .transpose(1, 2)\n",
    "            )\n",
    "            if self.cache_enabled:\n",
    "                self.cache[cache_key] = {\"k\": k, \"v\": v}\n",
    "\n",
    "        if self.training:\n",
    "            k = (\n",
    "                self.k(x)\n",
    "                .view(B, T, self.config.heads, C // self.config.heads)\n",
    "                .transpose(1, 2)\n",
    "            )\n",
    "            v = (\n",
    "                self.v(x)\n",
    "                .view(B, T, self.config.heads, C // self.config.heads)\n",
    "                .transpose(1, 2)\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            if cache_key in self.cache:\n",
    "                k, v = self.cache[cache_key][\"k\"], self.cache[cache_key][\"v\"]\n",
    "\n",
    "            else:\n",
    "                k = (\n",
    "                    self.k(x)\n",
    "                    .view(B, T, self.config.heads, C // self.config.heads)\n",
    "                    .transpose(1, 2)\n",
    "                )\n",
    "                v = (\n",
    "                    self.v(x)\n",
    "                    .view(B, T, self.config.heads, C // self.config.heads)\n",
    "                    .transpose(1, 2)\n",
    "                )\n",
    "\n",
    "                self.cache[cache_key] = {\"k\": k, \"v\": v}\n",
    "\n",
    "        q = (\n",
    "            self.q(x)\n",
    "            .view(B, T, self.config.heads, C // self.config.heads)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) / ((C // self.config.heads) ** 0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 1, float(\"-inf\"))\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        return self.out(x), attn\n",
    "\n",
    "\n",
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig, layer_idx):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                AttentionHead(config, layer_idx=layer_idx, head_idx=i)\n",
    "                for i in range(config.heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # input and output are the same size\n",
    "        attns = []\n",
    "        for head in self.heads:\n",
    "            attn, _ = head(x, mask=mask)\n",
    "            attns.append(attn)\n",
    "\n",
    "        return torch.mean(torch.stack(attns), dim=0)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPTConfig, layer_idx):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(config.emb_size)\n",
    "        self.attn = MaskedMultiHeadAttention(config, layer_idx)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(config.emb_size)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(config.emb_size, config.ff_mult * config.emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.ff_mult * config.emb_size, config.emb_size),\n",
    "        )\n",
    "\n",
    "        if config.ff_dropout > 0:\n",
    "            self.ff_dropout = nn.Dropout(config.ff_dropout)\n",
    "\n",
    "        if config.attn_dropout > 0:\n",
    "            self.attn_dropout = nn.Dropout(config.attn_dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        identity = x\n",
    "        x = self.ln1(x)\n",
    "        x = self.attn(x, mask=mask)\n",
    "\n",
    "        if hasattr(self, \"attn_dropout\"):\n",
    "            x = self.attn_dropout(x)\n",
    "\n",
    "        x = x + identity\n",
    "\n",
    "        identity = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.ff(x)\n",
    "\n",
    "        if hasattr(self, \"ff_dropout\"):\n",
    "            x = self.ff_dropout(x)\n",
    "\n",
    "        return x + identity\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.emb_size)\n",
    "        self.pos_emb = nn.Embedding(config.block_size, config.emb_size)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(config, layer_idx=i) for i in range(config.num_layers)]\n",
    "        )\n",
    "\n",
    "        self.ln = nn.LayerNorm(config.emb_size)\n",
    "        self.head = nn.Linear(config.emb_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie weights\n",
    "        self.head.weight = self.token_emb.weight\n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, std=0.02)\n",
    "\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "        assert (\n",
    "            not T > self.config.block_size\n",
    "        ), \"Sequence length is longer than block size\"\n",
    "\n",
    "        emb = self.token_emb(x)\n",
    "        pe = self.pos_emb(torch.arange(T - 1, -1, step=-1, device=device))\n",
    "        mask = torch.triu(torch.ones(T, T, device=device), diagonal=1).unsqueeze(0)\n",
    "\n",
    "        x = emb + pe\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask=mask)\n",
    "\n",
    "        x = self.ln(x)\n",
    "        return self.head(x)\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        # Input is a contiguous tensor\n",
    "        y = y.flatten()\n",
    "        y_pred = y_pred.view(-1, y_pred.size(-1))\n",
    "\n",
    "        return F.cross_entropy(y_pred, y)\n",
    "\n",
    "    def get_param_count(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        start_seq: str,\n",
    "        max_len: int = 128,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: int = 50,\n",
    "    ):\n",
    "        self.eval()\n",
    "\n",
    "        generated = tokenizer.encode(start_seq).ids\n",
    "        primer_t = torch.as_tensor(generated, device=device).view(1, -1)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            if primer_t.size(1) >= self.config.block_size:\n",
    "                primer_t = primer_t[:, -self.config.block_size :]\n",
    "\n",
    "            out = self(primer_t)\n",
    "            out = out[:, -1, :] / temperature\n",
    "            out = F.softmax(out, dim=-1)\n",
    "            out = torch.topk(out, top_k, dim=-1)[0]\n",
    "            out = torch.multinomial(out, num_samples=1)\n",
    "\n",
    "            gen = out.item()\n",
    "\n",
    "            generated.append(gen)\n",
    "\n",
    "            primer_t = torch.cat((primer_t, out), dim=1)\n",
    "\n",
    "        return tokenizer.decode(generated), generated\n",
    "\n",
    "\n",
    "config = GPTConfig()\n",
    "model = GPT(config).to(device)\n",
    "num_train_steps = 0\n",
    "\n",
    "print(f\"Model has {model.get_param_count():,} parameters\")\n",
    "print(model.generate(\"To be or not to be\", max_len=128)[0])\n",
    "\n",
    "del config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm.tqdm(dataloader, desc=\"Evaluation\")\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        loss = model.loss(y, y_pred).item()\n",
    "        total_loss += loss\n",
    "        pbar.set_postfix({\"loss\": loss})\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import math\n",
    "import torch.distributed as dist\n",
    "from torch.optim.optimizer import _dispatch_sqrt\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class Adam_mini(Optimizer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            model=None,\n",
    "            weight_decay=0.1,\n",
    "            lr=1,\n",
    "            beta1=0.9,\n",
    "            beta2=0.999,\n",
    "            epsilon=1e-8,\n",
    "            model_sharding=False,\n",
    "            n_embd=2048,\n",
    "            n_head=32,\n",
    "            n_query_groups=None\n",
    "    ):\n",
    "        '''\n",
    "        model: the model you are training.\n",
    "\n",
    "        model_sharding: set to True if you are using model parallelism with more than 1 GPU, including FSDP and zero_1,2,3 in Deepspeed. Set to False if otherwise.\n",
    "\n",
    "        n_embd: number of embedding dimensions. Could be unspecified if you are training non-transformer models.\n",
    "\n",
    "        n_head: number of attention heads. Could be unspecified if you are training non-transformer models.\n",
    "\n",
    "        n_query_groups: number of query groups in Group query Attention. If not specified, it will be equal to n_head. Could be unspecified if you are training non-transformer models.\n",
    "        '''\n",
    "\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        if n_query_groups is not None:\n",
    "            self.n_query_groups = n_query_groups\n",
    "            assert self.n_head % self.n_query_groups == 0\n",
    "        else:\n",
    "            self.n_query_groups = self.n_head\n",
    "\n",
    "        self.model = model\n",
    "        self.world_size = torch.cuda.device_count()\n",
    "        self.model_sharding = model_sharding\n",
    "        if self.model_sharding:\n",
    "            print(\"Adam-mini is using model_sharding\")\n",
    "        optim_groups = []\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                dic = {}\n",
    "                dic[\"name\"] = name\n",
    "                dic[\"params\"] = param\n",
    "                if (\"norm\" in name or \"ln_f\" in name):\n",
    "                    dic[\"weight_decay\"] = 0\n",
    "                else:\n",
    "                    dic[\"weight_decay\"] = weight_decay\n",
    "\n",
    "                if (\"self_attn.k_proj.weight\" in name or \"self_attn.q_proj.weight\" in name or \"attn.wq.weight\" in name or \"attn.wk.weight\"):\n",
    "                    dic[\"parameter_per_head\"] = self.n_embd * self.n_embd // self.n_head\n",
    "\n",
    "                if (\"attn.attn.weight\" in name or \"attn.qkv.weight\" in name):\n",
    "                    dic[\"n_head\"] = self.n_head\n",
    "                    dic[\"q_per_kv\"] = self.n_head // self.n_query_groups\n",
    "\n",
    "                optim_groups.append(dic)\n",
    "\n",
    "        defaults = dict(lr=lr, beta1=beta1, beta2=beta2, epsilon=epsilon)\n",
    "\n",
    "        super(Adam_mini, self).__init__(optim_groups, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for group in self.param_groups:\n",
    "                beta1 = group[\"beta1\"]\n",
    "                beta2 = group[\"beta2\"]\n",
    "                lr = group[\"lr\"]\n",
    "                name = group[\"name\"]\n",
    "                epsilon = group[\"epsilon\"]\n",
    "\n",
    "                for p in group[\"params\"]:\n",
    "                    state = self.state[p]\n",
    "                    if (\"embed_tokens\" in name or \"wte\" in name or \"lm_head\" in name):\n",
    "                        if p.grad is None:\n",
    "                            continue\n",
    "                        if len(state) == 0:\n",
    "                            state[\"m\"] = torch.zeros_like(p.data).to(torch.float32)\n",
    "                            state[\"iteration\"] = 0\n",
    "                            state[\"v\"] = torch.zeros_like(p.data).to(torch.float32)\n",
    "\n",
    "                        grad = p.grad.data.to(torch.float32)\n",
    "                        state[\"v\"].mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)\n",
    "                        state[\"iteration\"] += 1\n",
    "                        if group[\"weight_decay\"] != 0:\n",
    "                            p.data.mul_(1 - lr * group[\"weight_decay\"])\n",
    "\n",
    "                        state[\"m\"].lerp_(grad, 1 - beta1)\n",
    "\n",
    "                        bias_correction_1 = 1 - beta1 ** state[\"iteration\"]\n",
    "                        bias_correction_2 = 1 - beta2 ** state[\"iteration\"]\n",
    "                        bias_correction_2_sqrt = math.sqrt(bias_correction_2)\n",
    "\n",
    "                        h = (state[\"v\"].sqrt() / bias_correction_2_sqrt).add_(epsilon)\n",
    "                        stepsize = lr / bias_correction_1\n",
    "                        p.addcdiv_(state[\"m\"], h, value=-stepsize)\n",
    "\n",
    "                    elif (\n",
    "                            \"self_attn.k_proj.weight\" in name or \"self_attn.q_proj.weight\" in name or \"attn.wq.weight\" in name or \"attn.wk.weight\" in name):\n",
    "                        if p.grad is None:\n",
    "                            continue\n",
    "                        dim = group[\"parameter_per_head\"]\n",
    "                        if (len(state) == 0):\n",
    "                            state[\"m\"] = torch.zeros_like(p.data).to(torch.float32)\n",
    "                            state[\"m\"] = state[\"m\"].view(-1, dim)\n",
    "                            state['head'] = state['m'].shape[0]\n",
    "                            state[\"iteration\"] = 0\n",
    "                            state[\"vmean\"] = torch.zeros(state['head']).to(device)\n",
    "\n",
    "                        grad = p.grad.data.to(torch.float32)\n",
    "                        head = state['head']\n",
    "                        grad = grad.view(head, dim)\n",
    "\n",
    "                        tmp_lr = torch.mean(grad * grad, dim=1).to(device)\n",
    "                        state[\"vmean\"].mul_(beta2).add_(tmp_lr, alpha=1 - beta2)\n",
    "                        v = state[\"vmean\"]\n",
    "\n",
    "                        state[\"iteration\"] += 1\n",
    "                        if group[\"weight_decay\"] != 0:\n",
    "                            p.data.mul_(1 - lr * group[\"weight_decay\"])\n",
    "\n",
    "                        state[\"m\"].lerp_(grad, 1 - beta1)\n",
    "\n",
    "                        bias_correction_1 = 1 - beta1 ** state[\"iteration\"]\n",
    "                        bias_correction_2 = 1 - beta2 ** state[\"iteration\"]\n",
    "                        bias_correction_2_sqrt = math.sqrt(bias_correction_2)\n",
    "\n",
    "                        h = (v.sqrt() / bias_correction_2_sqrt).add_(epsilon)\n",
    "                        stepsize = ((1 / bias_correction_1) / h).view(head, 1)\n",
    "\n",
    "                        update = state[\"m\"] * (stepsize.to(state['m'].device))\n",
    "\n",
    "                        if p.dim() > 1:\n",
    "                            d0, d1 = p.size()\n",
    "                            update = update.view(d0, d1)\n",
    "                        else:\n",
    "                            update = update.view(-1)\n",
    "\n",
    "                        update.mul_(lr)\n",
    "                        p.add_(-update)\n",
    "\n",
    "                    elif (\"attn.attn.weight\" in name or \"attn.qkv.weight\" in name):\n",
    "                        if p.grad is None:\n",
    "                            continue\n",
    "                        if (len(state) == 0):\n",
    "                            state[\"m\"] = torch.zeros_like(p.data).to(torch.float32)\n",
    "                            state[\"m\"] = state[\"m\"].view(group[\"n_head\"], group[\"q_per_kv\"] + 2, -1)\n",
    "                            state[\"iteration\"] = 0\n",
    "                            state[\"vmean\"] = torch.zeros(group[\"n_head\"], group[\"q_per_kv\"] + 2).to(device)\n",
    "\n",
    "                        grad = p.grad.data.to(torch.float32)\n",
    "                        grad = grad.view(group[\"n_head\"], group[\"q_per_kv\"] + 2, -1)\n",
    "\n",
    "                        tmp_lr = torch.mean(grad * grad, dim=2).to(device)\n",
    "                        state[\"vmean\"].mul_(beta2).add_(tmp_lr, alpha=1 - beta2)\n",
    "                        v = state[\"vmean\"]\n",
    "\n",
    "                        print(f'name = {name} tmp_lr = {tmp_lr.size()}, vmean = {v.size()}')\n",
    "\n",
    "                        state[\"iteration\"] += 1\n",
    "                        if group[\"weight_decay\"] != 0:\n",
    "                            p.data.mul_(1 - lr * group[\"weight_decay\"])\n",
    "\n",
    "                        state[\"m\"].lerp_(grad, 1 - beta1)\n",
    "\n",
    "                        bias_correction_1 = 1 - beta1 ** state[\"iteration\"]\n",
    "                        bias_correction_2 = 1 - beta2 ** state[\"iteration\"]\n",
    "                        bias_correction_2_sqrt = math.sqrt(bias_correction_2)\n",
    "\n",
    "                        h = (v.sqrt() / bias_correction_2_sqrt).add_(epsilon)\n",
    "                        stepsize = ((1 / bias_correction_1) / h).view(group[\"n_head\"], group[\"q_per_kv\"] + 2, 1)\n",
    "\n",
    "                        update = state[\"m\"] * (stepsize.to(state['m'].device))\n",
    "\n",
    "                        if p.dim() > 1:\n",
    "                            d0, d1 = p.size()\n",
    "                            update = update.view(d0, d1)\n",
    "                        else:\n",
    "                            update = update.view(-1)\n",
    "\n",
    "                        update.mul_(lr)\n",
    "                        p.add_(-update)\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        if (len(state) == 0):\n",
    "                            dimension = torch.tensor(p.data.numel()).to(device).to(torch.float32)\n",
    "                            reduced = False\n",
    "                            if (self.world_size > 1) and (self.model_sharding is True):\n",
    "                                tensor_list = [torch.zeros_like(dimension) for _ in range(self.world_size)]\n",
    "                                dist.all_gather(tensor_list, dimension)\n",
    "                                s = 0\n",
    "                                dimension = 0\n",
    "                                for d in tensor_list:\n",
    "                                    if (d > 0):\n",
    "                                        s = s + 1\n",
    "                                    dimension = dimension + d\n",
    "                                if (s >= 2):\n",
    "                                    reduced = True\n",
    "\n",
    "                            state[\"m\"] = torch.zeros_like(p.data).to(torch.float32)\n",
    "                            state[\"iteration\"] = 0\n",
    "                            state[\"reduced\"] = reduced\n",
    "                            state[\"vmean\"] = torch.tensor(0.0).to(device)\n",
    "                            state[\"dimension\"] = dimension.item()\n",
    "                        if p.grad is None:\n",
    "                            tmp_lr = torch.tensor(0.0).to(device)\n",
    "                        else:\n",
    "                            grad = p.grad.data.to(torch.float32)\n",
    "                            tmp_lr = torch.sum(grad * grad).to(device)\n",
    "                        if (state[\"reduced\"]):\n",
    "                            dist.all_reduce(tmp_lr, op=dist.ReduceOp.SUM)\n",
    "                        if (p.grad is None):\n",
    "                            continue\n",
    "                        tmp_lr = tmp_lr / (state[\"dimension\"])\n",
    "                        tmp_lr = tmp_lr.to(grad.device)\n",
    "\n",
    "                        if group[\"weight_decay\"] != 0:\n",
    "                            p.data.mul_(1 - lr * group[\"weight_decay\"])\n",
    "                        state[\"iteration\"] += 1\n",
    "                        state[\"m\"].lerp_(grad, 1 - beta1)\n",
    "\n",
    "                        bias_correction_1 = 1 - beta1 ** state[\"iteration\"]\n",
    "                        bias_correction_2 = 1 - beta2 ** state[\"iteration\"]\n",
    "                        bias_correction_2_sqrt = math.sqrt(bias_correction_2)\n",
    "                        state[\"vmean\"] = (1 - beta2) * tmp_lr + beta2 * state[\"vmean\"]\n",
    "                        h = (state[\"vmean\"].sqrt() / bias_correction_2_sqrt).add_(epsilon)\n",
    "\n",
    "                        stepsize = (1 / bias_correction_1) / h\n",
    "                        update = state[\"m\"] * (stepsize.to(state['m'].device))\n",
    "                        update.mul_(lr)\n",
    "                        p.add_(-update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam_mini(model=model, lr=5e-4, weight_decay=0.01, n_embd=model.config.emb_size, n_head=model.config.heads)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=5)\n",
    "\n",
    "roll_loss = 0\n",
    "val_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    pbar = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch + 1}\", leave=True)\n",
    "    if roll_loss == 0:\n",
    "        pass\n",
    "    else:\n",
    "        val_loss = evaluate(model, val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    model.train()\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = model.loss(y, y_pred)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        num_train_steps += 1\n",
    "        roll_loss = 0.9 * roll_loss + 0.1 * loss.item()\n",
    "\n",
    "        pbar.set_postfix_str(f\"loss: {roll_loss:.4f}, val_loss: {val_loss:.2e}, steps: {num_train_steps:,}\")\n",
    "\n",
    "        # assert num_train_steps != 100, \"Stop training\"\n",
    "\n",
    "    scheduler.step(roll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.grad.mean().item():4f}, {param.grad.std().item():4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    print(model.generate(\n",
    "        \"The Project Gutenberg eBook\",\n",
    "        max_len=128,\n",
    "        \n",
    "        top_k=1000\n",
    "    )[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot postion embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pos_emb = model.pos_emb.weight.detach().cpu().numpy()\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.imshow(pos_emb, aspect=\"auto\", cmap=\"RdYlGn\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Position Embeddings\")\n",
    "plt.xlabel(\"Embedding Dimension\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(model.pos_emb.weight.detach().cpu().numpy()[:, 0])\n",
    "plt.title(\"Position Embedding 0\")\n",
    "plt.xlabel(\"Position\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot attention heads\n",
    "x, y = next(iter(train_loader))\n",
    "x, y = x.to(device), y.to(device)\n",
    "\n",
    "model.eval()\n",
    "B, T = x.size()\n",
    "\n",
    "emb = model.token_emb(x)\n",
    "pe = model.pos_emb(torch.arange(T - 1, -1, step=-1, device=device))\n",
    "\n",
    "x = emb + pe\n",
    "\n",
    "attns = []\n",
    "for block in model.blocks:\n",
    "    x = block.ln1(x)\n",
    "    for head in block.attn.heads:\n",
    "        attn, _ = head(x)\n",
    "        attns.append(attn)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.imshow(attns[i][:, :, 0].detach().cpu().numpy(), aspect=\"auto\", cmap=\"RdYlGn\")\n",
    "    plt.title(f\"Attention Head {i}\")\n",
    "    plt.xlabel(\"Query Position\")\n",
    "    plt.ylabel(\"Key Position\")\n",
    "\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
