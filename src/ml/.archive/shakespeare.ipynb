{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stetup\n",
    "\n",
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"blog/5-shakespeare/data/train.txt\", \"r\") as f:\n",
    "    train_corpus = f.read()\n",
    "\n",
    "with open(\"blog/5-shakespeare/data/test.txt\", \"r\") as f:\n",
    "    test_corpus = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, decoders, trainers, tools, pre_tokenizers\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "trainer = trainers.BpeTrainer(special_tokens=[\"[PAD]\", \"[SOS]\", \"[EOS]\", \"[MASK]\", \"[UNK]\"], vocab_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train([\"blog/5-shakespeare/data/train.txt\"], trainer=trainer)\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = tools.EncodingVisualizer(tokenizer)\n",
    "viz(train_corpus[:512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded_corpus = tokenizer.encode(train_corpus).ids\n",
    "val_encoded_corpus = tokenizer.encode(test_corpus).ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 256\n",
    "batch_size = 128\n",
    "\n",
    "# Reshape the encoded corpus into a tensor\n",
    "train_tensor = torch.tensor([train_encoded_corpus], dtype=torch.long).to(device)\n",
    "val_tensor = torch.tensor([val_encoded_corpus], dtype=torch.long).to(device)\n",
    "\n",
    "# Make sure its multple of seq_len\n",
    "train_tensor = train_tensor[:, :train_tensor.size(1) // seq_len * seq_len].view(-1, seq_len)\n",
    "val_tensor = val_tensor[:, :val_tensor.size(1) // seq_len * seq_len].view(-1, seq_len)\n",
    "print(train_tensor.size(), val_tensor.size())\n",
    "\n",
    "# Create a dataset\n",
    "train_dataset = torch.utils.data.TensorDataset(train_tensor)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_tensor)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, dropout, batch_first=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers,dropout=dropout, batch_first=batch_first)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden: tuple[torch.Tensor, torch.Tensor] | None = None):\n",
    "        if hidden is None:\n",
    "            hidden = (\n",
    "                torch.zeros(self.rnn.num_layers, x.size(0), self.rnn.hidden_size, device=x.device),\n",
    "                torch.zeros(self.rnn.num_layers, x.size(0), self.rnn.hidden_size, device=x.device),\n",
    "            )\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x, hidden\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        start_seq: str,\n",
    "        max_len: int = 128,\n",
    "        hidden: tuple[torch.Tensor, torch.Tensor] | None = None,\n",
    "    ) -> str:\n",
    "        self.eval()\n",
    "        start_seq = tokenizer.encode(start_seq).ids\n",
    "\n",
    "        if hidden is None:\n",
    "            hidden = (\n",
    "                torch.randn(self.rnn.num_layers, 1, self.rnn.hidden_size, device=device),\n",
    "                torch.randn(self.rnn.num_layers, 1, self.rnn.hidden_size, device=device)\n",
    "            )\n",
    "        \n",
    "        x = torch.tensor(start_seq, dtype=torch.long, device=device).view(1, -1)\n",
    "        output = x.flatten().tolist()\n",
    "        for _ in range(max_len):\n",
    "            x, hidden = self(x, hidden)\n",
    "            if x.shape[1] > 1:\n",
    "                x = x[:, -1:]\n",
    "\n",
    "            x = x.softmax(dim=-1).argmax(dim=-1)\n",
    "            if x.item() == tokenizer.token_to_id(\"[EOS]\"):\n",
    "                break\n",
    "            output = output + x.flatten().tolist()\n",
    "        self.train()\n",
    "        \n",
    "        return tokenizer.decode(output)\n",
    "\n",
    "\n",
    "hidden_size = 512\n",
    "num_layers = 4\n",
    "dropout = 0.0\n",
    "\n",
    "model = LSTM(tokenizer.get_vocab_size(), hidden_size, num_layers, dropout).to(device)\n",
    "num_train_steps = 0\n",
    "\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "# model.generate()\n",
    "model.generate(\"The Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 19\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m \u001b[38;5;241m*\u001b[39m train_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem(), roll_loss\u001b[38;5;241m=\u001b[39mtrain_loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = 0\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    pbar = tqdm.tqdm(train_loader, leave=False, desc=f\"Epoch {epoch + 1}\")\n",
    "    for x, in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(device)\n",
    "        inputs = x[:, :-1]\n",
    "        targets = x[:, 1:]\n",
    "\n",
    "        output, _ = model(inputs)\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        targets = targets.flatten()\n",
    "\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss = 0.9 * train_loss + 0.1 * loss.item()\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item(), roll_loss=train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.generate(\"The Project\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
